=== GENERATED PROMPT ===

You are an information verifier.
You are given a json object and a reference document, your task is to score the information (key, value pair) presented in the extraced JSON object based on the information presented in the reference document.

=== START OF EXPLANATION OF WHAT EACH FIELD IN THE JSON MEAN ===
{
  "original_study": {
    "claim": {
      "hypotheses": "A testable hypothesis based on the claim",
      "hypotheses_location": "Specific location where the hypotheses is stated in the original paper",
      "statement": "The main claim made by the original study that matches the claim given as an input.",
      "statement_location": "Specific location where the claim is stated in the original paper.",
      "study_type": "Type of study (Experimental, Observational, Meta-Analysis)"
    },
    "data": {
      "source": "Data Source (e.g., survey, database)",
      "wave_or_subset": "Specific waves or subsets (if applicable)",
      "sample_size": "Sample size of the selected data (if applicable)",
      "unit_of_analysis": "Unit of analysis (e.g., individual, household)",
      "access_details": "Access details (e.g., restrictions, request process)",
      "notes": "Any additional information or caveats (e.g., encoding issues, nested structure, missing metadata, unusual column formats)"
    },
    "method": {
      "description": "Narrative summary of how the study was conducted.",
      "steps": "Ordered list of procedural steps to reproduce the study.",
      "models": "Models or statistical approach (e.g., regression type)",
      "outcome_variable": "Dependent/outcome variable measured or analyzed",
      "independent_variables": "Primary variables expected to influence the outcome",
      "control_variables": "Variables controlled for in the analysis",
      "tools_software": "Tools or software specifics (e.g., mentions of R, Python, packages)"
    },
    "results": {
      "summary": "Narrative summary of the main results or findings from the original study.",
      "numerical_results": [
        {
          "outcome_name": "Name or label of the outcome (e.g., 'test_score', 'conversion_rate')",
          "value": "Numeric result value (e.g., 0.45)",
          "unit": "Optional unit of measurement (e.g., %, points, ms)",
          "effect_size": "Optional effect size (e.g., Cohen's d, odds ratio)",
          "confidence_interval": {
            "lower": "Lower bound (e.g., 0.32)",
            "upper": "Upper bound (e.g., 0.58)",
            "level": "Confidence level, e.g., 95"
          },
          "p_value": "Optional p-value associated with the result (e.g., 0.001)",
          "statistical_significance": "Boolean indicating if result is statistically significant",
          "direction": "Qualitative direction of the effect (positive, negative, or null) based on the sign and magnitude of the reported result."
        }
      ]
    },
    "metadata": {
      "original_paper_id": "DOI or unique identifier.",
      "original_paper_title": "Title of the original paper.",
      "original_paper_code": "Link to the original study's codebase.",
      "original_paper_data": "Link to the original study's dataset(s)."
    }
  }
}
=== END OF EXPLANATION OF WHAT EACH FIELD IN THE JSON MEAN ===

Follow the rubrics below for your evaluation of each component in the JSON. The rubric uses a 0-3 scoring scale for all components, where:
- 3: Exact Match – The extracted information is identical or nearly identical to the reference in the document. The meaning is preserved completely, with the same level of detail, including all key elements (e.g., variables, relationships, or numerical values). No omissions or additions of unrelated information. 
- 2: Mostly Similar – The extracted information conveys the same core meaning as the reference, but with different phrasing or structure. Minor details may be omitted, but the essential information is preserved. If the extracted content is a finding that directly supports a hypothesis, consider it equivalent. 
- 1: Loosely Related – The extracted information has partial overlap with the reference but includes significant differences, omissions of major details, or additions of unrelated information. The core meaning is somewhat preserved but incomplete or altered.
- 0: No Match – No relevant overlap with the reference, completely incorrect, or missing entirely.
- NA: The reference document does not contain information to evaluate this component/field of the JSON.

=== START OF EXTRACTED JSON TO BE EVALUATED START ===
    {
  "replication_study": {
    "hypothesis": "There is an inverted U-shaped relationship between local electoral violence and measured election fraud: at low-to-moderate levels of violence district/polling-center-level fraud increases with violence but declines at high levels (operationalized as a positive coefficient on the linear violence term and a negative, statistically significant coefficient on the squared violence term in a logit model predicting fraud indicator).",
    "study_type": "Observational",
    "data_plan": {
      "dataset_identifier": "Afghanistan_Election_Violence_2014.csv (provided in replication_data)",
      "source_type": "Prepared tabular dataset (CSV) of polling-center/district-level election returns, violence measures, and covariates",
      "wave_or_subset": "Full provided CSV (all rows). The file path is data/original/15/0205_gpt5-mini/replication_data/Afghanistan_Election_Violence_2014.csv",
      "sample_size": "800 rows (as loaded from the CSV: see dataset shape). See data/original/15/0205_gpt5-mini/replication_data for the file.",
      "unit_of_analysis": "Row-level entries in the provided CSV. Variables include province, district, distid, and polling-center identifiers (pcn / pcno) suggesting the unit is polling-center or district-observation depending on file's construction; the dataset contains 25 columns including fraud, sigact_5r, sigact_60r, pcx, electric, pcexpend, dist, elevation, and regcom.",
      "access_details": "Data file is provided locally in the repository at data/original/15/0205_gpt5-mini/replication_data/Afghanistan_Election_Violence_2014.csv; no external access restrictions for this copy.",
      "qualification": {
        "explanation": "This provided CSV contains the core variables needed to attempt a replication of the focal claim (fraud indicator and violence measures plus controls). The Stata .do file that came with the replication_data uses the same variable names (e.g., 'fraud', 'sigact_5r', 'sigact_60r', 'pcx', 'electric', 'pcexpend', 'dist', 'elevation', 'regcom'), which enabled a high-fidelity translation of the original analysis steps into Python. See the .do script used as the basis for translation: data/original/15/0205_gpt5-mini/replication_data/Weidmann_Data_Analysis_Final.do.",
        "similarity_to_original": "The variables and regression specification in the .do file match the available CSV column names (confirmed via inspection). The original analysis described in post_registration.json indicates the main independent variables and controls are sigact_5r (5-day violence), sigact_60r (60-day violence), their squared terms, pcx (percentage of centers closed), electric (electrification), pcexpend (per-capita expenditure), dist (distance from Kabul), and elevation. These variables are present in the CSV. Evidence: - Variable names present in CSV: see data/original/15/0205-gpt5-mini/replication_data/Afghanistan_Election_Violence_2014.csv (columns list returned when loading dataset). - Original study data/variables described in: data/original/15/0205_gpt5-mini/post_registration.json under original_study.data and original_study.method (see 'independent_variables' and 'control_variables'). - The Stata script uses exactly these variable names: data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final.do (lab var and model lines).",
        "deviation_from_original": "Important deviations that make this a different sample/replication: - The original paper (post_registration.json) reports district-level analyses for the 2009 Afghanistan presidential election (district sample sizes ~398). The CSV filename includes '2014' (Afghanistan_Election_Violence_2014.csv), and the CSV contains 800 rows and polling-center identifiers (pcn / pcno columns). This suggests the provided dataset is at a finer aggregation (polling-center level) or a different election wave/year than the district-level 2009 sample in the original paper. (Cite: dataset file path and post-registration description: data/original/15/0205_gpt5-mini/post_registration.json which states original used 2009 and district-level N~398.) - Because the unit and/or year may differ, the replication tests robustness of the hypothesis in an independent sample and unit of analysis rather than a strict exact-data replication. The presence of regcom in the CSV allows clustering by regional command as in the original .do, preserving an important inferential aspect (see .do script at data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final.do)."
      },
      "notes": "Potential data issues and caveats: - The original .do runs logit with 'fraud' as the outcome; we verified a column named 'fraud' exists in the CSV, but its coding should be inspected carefully (binary vs continuous). The provided Python script coerces to a binary fraud indicator (fraud_bin = fraud > 0) if the variable is not already binary. This is noted because the .do assumes a binary outcome for logit. See the created script: data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final__py.py. - The data include both sigact_5r and sigact_60r; squared terms must be computed. - The CSV has 25 columns and 800 rows; original models reported smaller district-sample sizes; this difference should be addressed in analysis and interpretation. - Missing-data handling will use listwise deletion for the model variables (detailed below)."
    },
    "planned_method": {
      "steps": [
        "1) Inspect the CSV in /app/data/.../replication_data for variable coding (particularly 'fraud'), verify binary coding or convert to binary indicator if needed. (I have already implemented a conservative conversion in the Python script: fraud_bin = (fraud > 0)).",
        "2) Compute squared terms for sigact_5r and sigact_60r (sigact_5r_sq, sigact_60r_sq).",
        "3) Drop rows with missing values in variables required for each model (listwise deletion for those model covariates).",
        "4) Estimate logit models mirroring the .do: Model 1 (5-day window): fraud ~ sigact_5r + sigact_5r^2 + controls (pcx, electric, pcexpend, dist, elevation), clustered SEs by regcom if available.",
        "5) Estimate Model 1 with multiway clustering where possible (original uses regcom and elect clusters) or report robust SEs if multiway clustering not available in current environment.",
        "6) Estimate Model 2 (60-day window): fraud ~ sigact_60r + sigact_60r^2 + same controls, clustered SEs as above.",
        "7) Compute margins / predicted probabilities across a grid of violence values for each model and produce plots (margins and marginsplot equivalents saved to CSV/graph files).",
        "8) Save coefficient tables, summaries and margins outputs to /app/data/original/15/0205_gpt5-mini/replication_data for inspection and comparison to original reported coefficients (especially the sign and significance of the squared violence term).",
        "9) Conduct robustness checks where feasible (e.g., alter fraud thresholding, alternative clustering, sample subsets, polling-center vs aggregated district-level if aggregation is possible).",
        "10) Compare the key coefficients (linear and squared violence terms), their standard errors, and p-values to the original figures and interpret concordance or deviations."
      ],
      "models": "Primary: logistic regression (logit) estimating probability of detected fraud. Regression specification includes quadratic term for violence (c.sigact_5r##c.sigact_5r and c.sigact_60r##c.sigact_60r in Stata; equivalently include linear and squared terms in Python). Cluster-robust SEs are used (one-way clustering by regcom as in original .do; where feasible test multiway clustering).",
      "outcome_variable": "fraud (binary indicator). In the provided script this is represented as 'fraud_bin' when coercion is necessary. The .do treated 'fraud' as the dependent variable in logit models (see Weidmann_Data_Analysis_Final.do).",
      "independent_variables": "Primary: sigact_5r (violence in 5-day election window) and its square; alternative: sigact_60r (60-day window) and its square. Also margins across these variables will be computed to visually assess inverted U-shape.",
      "control_variables": "pcx (percentage of centers closed), electric (electrification), pcexpend (per-capita expenditure), dist (distance from Kabul), elevation.",
      "tools_software": "Python 3.10+; key libraries: pandas, numpy, statsmodels. The translation from the Stata .do is implemented in a Python script created at data/original/15/0205_gpt5-mini/replication_data/Weidmann_Data_Analysis_Final__py.py.",
      "planned_estimation_and_test": {
        "estimation": "Estimated coefficients for linear and squared violence terms from logit models; predicted probabilities (margins) across violence grid points.",
        "test": "Two-sided Wald tests (z-statistics / p-values from logit outputs) for coefficients; substantive test is whether squared-violence coefficient is negative and statistically significant (e.g., p < 0.05). For margins, inspect the shape (increase then decrease) to confirm inverted U."
      },
      "missing_data_handling": "Listwise deletion for rows with missing data on variables used in each model. Report number of observations used in each estimation and check robustness to alternative imputation (if needed).",
      "multiple_testing_policy": "There are a small number of pre-specified primary tests (5-day and 60-day windows). No broad multiple testing correction planned for these pre-specified primary hypotheses; if many exploratory tests are run, apply FDR (Benjamini-Hochberg) correction or Bonferroni for secondary analyses and report both corrected and uncorrected p-values.",
      "inference_criteria": "Primary criterion: coefficient on squared violence term negative and p < 0.05 (two-sided). Also check sign of linear term consistent with an initial increase and examine margins to confirm inverted-U shape visually and numerically (peak location within observed range). Report effect sizes and confidence intervals. Note that causal claims are limited; replicating the observed pattern is a robustness test."
    },
    "codebase": {
      "files": {
        "replication_data/Weidmann_Data_Analysis_Final__py.py": "Python translation and implementation of the primary analysis in the original Stata .do. Loads /app/data/original/15/0205_gpt5-mini/replication_data/Afghanistan_Election_Violence_2014.csv, computes squared terms, coerces fraud to a binary indicator if needed (fraud_bin), fits two logit models (5-day and 60-day windows), computes cluster-robust standard errors (grouping by regcom where available), produces margin predictions across grids, and writes outputs to the replication_data directory (model summaries, coefficient CSVs, margins CSVs, dataset_info.json).",
        "replication_data/Afghanistan_Election_Violence_2014.csv": "Primary dataset used for estimation. Must be present in /app/data at runtime. Contains columns required by the analysis (fraud, sigact_5r, sigact_60r, pcx, electric, pcexpend, dist, elevation, regcom, etc.).",
        "replication_data/Weidmann_Data_Analysis_Final.do": "Original Stata script included as reference. Not executed directly in the Python-based replication run; used to guide model translation. Located at data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final.do.",
        "replication_data/model_5_summary.txt, model_60_summary.txt, model_5_coefs.csv, model_60_coefs.csv, margins_sigact_5r.csv, margins_sigact_60r.csv, dataset_info.json": "Output files produced by executing the Python script; contain model summaries, coefficient tables, predicted margins for inspection and comparison to original reported effects, and dataset metadata (row/column counts)."
      },
      "notes": "Design notes and dependencies: - The Python script uses statsmodels for logit estimation and robust covariance estimation. It attempts to emulate the Stata .do logic (including clustering by 'regcom' where possible). - Multiway clustering (two-way) is not directly implemented in statsmodels by a single call; if required, the replication plan includes reporting HC1 cluster-robust SEs as fallback or employing available multiway clustering helper functions. - All IO paths are absolute and point to /app/data to comply with run policy. - The original Stata .do is preserved and included for audit and cross-checking but the executed entrypoint is the Python script listed above. - The Python script file was created at data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final__py.py and should be executed in an environment where /app/data is mounted such that the path used in the script resolves correctly."
    },
    "docker_specs": {
      "base_image": "python:3.10-slim",
      "packages": {
        "python": [
          "pandas>=1.4.0",
          "numpy>=1.23.0",
          "statsmodels>=0.13.0",
          "scipy>=1.8.0",
          "matplotlib>=3.5.0 (optional, for plotting)",
          "openpyxl>=3.0.0 (optional if exporting Excel)"
        ],
        "other": [
          "git",
          "wget",
          "unzip"
        ]
      },
      "hardware": {
        "gpu_support": "false",
        "min_gpu_memory_gb": "0",
        "min_ram_gb": "4"
      },
      "volumes": [
        "./data:/app/data"
      ]
    },
    "analysis": {
      "instructions": "1) Ensure the repository is available at host path mounted to the container as ./data -> /app/data. 2) Install the Python dependencies listed in docker_specs (pip install pandas numpy statsmodels scipy). 3) From the container, run: python /app/data/original/15/0205-gpt5-mini/replication_data/Weidmann_Data_Analysis_Final__py.py 4) Inspect the generated output files in /app/data/original/15/0205-gpt5-mini/replication_data: - model_5_summary.txt and model_60_summary.txt for regression outputs and SEs - model_5_coefs.csv and model_60_coefs.csv for coefficient/p-value tables - margins_sigact_5r.csv and margins_sigact_60r.csv for the predicted probabilities across violence grids - dataset_info.json for dataset shape and column list 5) Compare the linear and squared violence term coefficients and p-values to the original reported values (particularly check sign and significance of squared term). 6) If needed, perform additional aggregations (e.g., aggregate polling-center level to district level) and re-run models to approximate original unit of analysis; document any changes and their impact.",
      "comparison_metrics": "Primary comparison metrics: - Coefficient sign and magnitude for the linear violence term and the squared-violence term. - Standard errors and p-values for those coefficients (one-sided interest in squared term negative; report two-sided p-values). - Predicted probability curves from margins: presence of a clear peak (increase then decrease) within the observed range of violence. - Sample size and unit-of-analysis differences (report N used in each model) compared versus original reported Ns. - If possible, a tabular comparison of original reported coefficients (from post_registration.json or original paper) to replication coefficients with \u0394 and percent difference."
    }
  }
}
=== END OF EXTRACTED JSON TO BE EVALUATED END ===
    
=== START OF REFERENCE DOCUMENT WITH THE CORRECT INFO ===
     
Replication of a Research Claim from Weidmann & Callen (2013), 
from the British Journal of Political Science 
 
Replication Team: Eric L. Sevigny and Jared Greathouse 
 
Research Scientist: Andrew Tyner 
 
Action Editor: Kevin Esterling 
 
Independent Reviewer: 
 
Reviewer #1: Landon Schnabel 
 
Review Period: June 24 - June 29 
 
View-only links to: Original Paper, Original Materials, Replication Materials, Replication 
Analyses 
 

 
Replication of a Research Claim from 
Weidmann & Callen (2013), from British 
Journal of Political Science 
 
SCORE report Weidmann_BritJournPoliSci_2013_JRpA_mk67 
 
 
Sourcing notes for this preregistration are here. 
 
 
 
Privacy Statement: Other teams are making predictions about the outcomes of many different 
studies, not knowing which studies have been selected for replication. As a consequence, the 
success of this project requires full confidentiality of the research process, including peer review. 
This includes privacy about which studies have been selected for replication and all aspects of 
the discussion about these replication designs. 
 
Table of contents 
Table of contents 
Sourcing Notes 
Preregistration for SCORE 
Study Information 
1. Title 
2. Authors and affiliations 
3. Description of study 
4. Hypotheses 
Design Plan 
5. Study type 
6. Blinding 
7. Blinding, additional details 
8. Study design 

 
9. Randomization 
Sampling Plan 
10. Existing data 
11. Explanation of existing data 
12. Data collection procedures 
13. Sample size 
15. Stopping rule 
Variables 
16. Manipulated variables 
17. Measured variables 
18. Indices 
Analysis Plan 
19. Statistical models 
20. Transformations 
21. Inference criteria 
22. Data exclusion 
23. Missing data 
24. Exploratory analysis 
25. Other 
References 
 
 
 
 
 
 
 
 

 
Sourcing Notes 
The research scientists write a short set of constraints or notes about each study that are designed 
to help match the replication/reproduction to a team that will perform it. They are maintained as 
part of the project record. 
 
No sourcing notes provided. 
 
 
 

 
Preregistration for SCORE 
Study Information 
1. Title  
 
Replication of a research claim from Weidmann & Callen (2013) in British Journal of Political 
Science. 
 
2. Authors and affiliations   
 
Eric L. Sevigny, PhD1 
Jared Greathouse, BA1 
 
1 Georgia State University 
 
3. Description of study  
 
The claim selected for replication from Weidmann & Callen (2013) is that the relationship between 
(in)security and election fraud should be in the form of an inverted U-shape. Fraud increases with 
violence up to a certain level, but then decreases again (Hypothesis 1). This reflects the following 
statement from the paper's abstract: "It predicts (i) that the relationship between violence and 
fraud follows an inverted U-shape and (ii) that loyalty networks of both incumbent and challenger 
react differently to the security situation on the ground. Disaggregated violence and election 
results data from the 2009 Afghanistan presidential election provide empirical results consistent 
with this theory." The claim is tested with the logit models presented in Table 2. The dependent 
variable is a binary measure of fraud, calculated by grouping polling stations by district and 
applying the Beber–Scacco last-digit test to the total vote count. The dependent variable takes 
the value of 1 if this test is significant at the 5 percent level for a particular district. The authors 
measure violence as the number of attacks per capita against the International Security 
Assistance Force (ISAF). In Model 1 (the model selected as evidence for the SCORE program), 
it is included in both linear and squared form. In conjunction with a positive and significant 
coefficient for the linear term, the claim is tested with the coefficient on the squared term (see 
Model 1 for specification details). Model 1 confirms Hypothesis 1, since the linear term of the 
violence measure receives a positive and significant coefficient, and the squared term a negative 
one (coefficient on Violence (election, squared) term = -13.748, SE clustered at the regional 
command level = 4.720, p < 0.01). 
 

 
4. Hypotheses 
 
H1: The linear association between violence and election fraud will be positive.  
H* (SCORE focal test): The quadratic association between violence and election fraud will be 
negative. 
 
The SCORE focal test references the quadratic term in the original model (Table 2, Model 1). The 
alternative hypothesis (H1) references the linear term in the same model, which is consistent with 
Hypothesis 1 as specified by Weidmann and Callen (2013): “The relationship between (in)security 
and election fraud should be in the form of an inverted U-shape. Fraud increases with violence 
up to a certain level, but then decreases again.” Testing the combination of H1 and H* will more 
fully assess this broader claim posed in the original study.  
 
Design Plan 
5. Study type 
 
Observational Study - Data is collected from study subjects that are not randomly assigned to a 
treatment. This includes surveys, natural experiments, and regression discontinuity designs. 
 
6. Blinding 
 
No blinding is involved in this study. 
 
7. Blinding, additional details 
 
N/A 
 
8. Study design 
 
This replication study will employ a cross-sectional observational research design to examine the 
association between insurgent violence and election fraud. Consistent with Weidmann and Callen 
(2013), this design explores the general pattern of fraud in response to violence, which cannot 
produce a strong causal test of the predicted association due to the lack of exogenous variation.  
 

 
The study setting remains Afghanistan as in the original study, but the current study will analyze 
returns from the 2014 Presidential election to replicate Weidmann and Callen’s (2013) analysis 
of the 2009 Presidential election. It should be noted that since no candidate won a majority of 
votes in the April 5, 2014 election, a runoff election was held on June 14, 2014 between the top 
two vote-getters (Abdullah Abdullah, Ashraf Ghani). We focus on the pooled voting results for 
both elections in this replication to increase power for the study. The unit of analysis remains the 
district, which is the second-level administrative unit in Afghanistan below the province.  
9. Randomization 
 
N/A 
Sampling Plan 
10. Existing data 
 
Registration will occur prior to analysis of the data 
 
11. Explanation of existing data 
 
This study will collect, aggregate, and merge secondary data from various sources to construct a 
replication analysis data file. With respect to the original study, all data used for the replication 
reflects the updated time frame (i.e., 2014 vs. 2009).  
12. Data collection procedures 
 
Secondary data collection procedures for the replication analysis will be consistent with the 
original study. In particular, secondary data will be obtained from the following sources. Data for 
the 2014 Afghanistan presidential election have been made publicly available here by the National 
Democratic Institute (NDI), a nonprofit, nonpartisan, nongovernmental organization based in 
Washington, DC. Though the replication data purveyor (National Democratic Institute) is different 
from the original study, the source of the data is the same (International Election Commission of 
Afghanistan). NDI data are sourced for this project because they are provided in a more 
accessible digitized form. The election data provide information on election returns, polling 
centers, province and district names, and geographic coordinates of districts. Data on violent 
insurgent activity in Afghanistan have been digitized from declassified ‘significant activity’ reports 
released by US Central Command and made publicly available here by researcher Vincent Bauer 
(SigActs). Again, although sourced differently from the original study, the data are similarly 
derived from military significant activity reports. Development data from the 2013-2014 
Afghanistan Living Conditions Survey (ALCS) can be requested here from the Afghanistan 

 
government’s National Statistics and Information Authority (NSIA). The ALCS is the successor to 
the 2007 National Risk and Vulnerability Assessment (NRVA) survey used in the original study. 
Elevation data (mean district elevation) comes from the World Bank here, which was derived from 
enhanced land elevation data produced by NASA's Shuttle Radar Topography Mission (SRTM). 
The original study used elevation data aggregated to the district level based on the US Geological 
Survey. Because elevation for nine districts was not reported in the World Bank data, we manually 
searched for elevation information by district latitude and longitude coordinates using Google 
Earth. Thus, for these nine districts, elevation is measured at one point on the map rather than as 
a district average from multiple locations. Distance to Kabul from each district is measured as the 
great circle distance on a spherical earth, identical to the original study. This distance measure 
was calculated within Stata using the -geodist- command based on latitude and longitude 
coordinates. Finally, population data have been formatted from official Afghan sources and made 
publicly available by researcher Colin Cookman here. Population data in the original study were 
based on the LandScan Global Population database.  
13. Sample size 
 
The unit of analysis is the district. For the 2009 Afghanistan election, Weidmann and Callen (2013) 
analyzed data from 398 districts (with a corresponding analytic sample of N=375 due to missing 
data). Various official district maps have been issued since then, some with as many as 422 
districts (see here for more background information). Based on these updated district maps and 
election reporting results, the current replication is based on a 408 district set. However, election 
results were returned from just 399 districts for the first election and 401 districts for the runoff 
election. Thus, because we are pooling first and runoff election results for this replication, the 
maximum analytic sample is N=800. Since the unit of analysis in this replication is finite and 
already maximized, there will be no planned second-stage supplemental data collection and 
analysis with a larger sample.   
 
 
14. Sample size rationale 
 
Sample size is predetermined by the study design and context.  
 
Power calculations were done in accordance with the guidelines of the Social Sciences 
Replication Project (SSRP). The first round of data collection aims to achieve 90% power to detect 
75% of the original effect size. A second-round sample, although not planned as part of this 
replication, would achieve 90% power to detect 50% of the original effect size. 
 
The 
power 
calculation 
for 
this 
replication 
can 
be 
found 
here: 
https://osf.io/n9bhj/?view_only=4d9fa88a0e504261a55875f329cdac1b. 
 

 
15. Stopping rule 
 
The target sample size is N=800, which reflects actual district-level election returns across the 
first (N=399) and runoff (N=401) elections. This number more than doubles the original study’s 
maximum sample size of N=398 because of an increase in the number of districts and pooling of 
2014 presidential initial and runoff election results. There is no plan for a second stage of data 
collection, and thus there is no stopping rule.  
 
Variables 
16. Manipulated variables 
 
There are no manipulated variables in this replication study.  
 
17. Measured variables 
 
Data 
cleaning 
and 
merging 
operations 
are 
provided 
in 
a 
Stata 
do 
file 
(Weidmann_Data_Cleaning.do) on the OSF project page. The final analytic dataset 
(Afghanistan_Election_Violence_2014.dta) is also posted on the OSF project page.  
  
Dependent Variable: Election Fraud 
Election data are reported at the polling station level (i.e., ballot collection points), which are 
nested within polling centers (e.g., mosque, school) within districts. As with the original study, 
voting data at the polling station level will be used to develop district-level measures of election 
fraud. We will use Beber and Scacco’s (2012) ‘last-digit measure’ methodology to create a district-
level binary dependent variable, where evidence of election fraud = ‘1’ and lack of such evidence 
= ‘0.’ In particular, election fraud is indicated when the within-district distribution of the last digit 
among polling station vote totals deviates significantly (p<0.05) from the expected uniform 
distribution. The use of Beber and Scacco’s (2012) last-digit method exactly replicates the 
operationalization in the original study.  
 
Focal Independent Variable: Violence 
Weidmann and Callan (2013) measured district-level insurgent violence as “the number of attacks 
per capita against the International Security Assistance Force (ISAF), [defined] as the total of 
insurgent-initiated direct fire incidents, incidents involving improvised explosive devices (IEDs), 
and small arms fire [occurring] in a five-day window around election day.” Like Weidmann and 
Callen (2013), replication data on insurgent violence in Afghanistan come from declassified 
military ‘significant activity’ or SigAct reports released by US Central Command. The data used 
for the replication have been collected, digitized, and made publicly available by researcher 
Vincent Bauer. These data comprise N=431,547 geographically and temporally referenced events 

 
for the period January 2008 to December 2014. The replication study will follow Weidmann and 
Callen’s (2013) operationalization of insurgent violence, defined as the number of insurgent 
violent events occurring in a five-day window surrounding election day (i.e., April 5-9, 2014 for 
first election and June 14-18, 2014 for runoff election) at the district level. Similar to the original 
study, insurgent violent events are defined as enemy actions involving (i) assassinations, direct 
fire, or small arms fire, or (ii) IED/mine explosive hazards.  
 
In addition to the main results reported in Table 2, Model 1, Weidmann and Callen (2013) also 
reported in Table 2, Model 2 a supplemental analysis measuring violence during a wider sixty-
day window prior to election day. We will replicate this supplemental analysis using the extended 
window of observation. Consistent with Weidmann and Callen (2013), both violence measures 
will be aggregated to the district level and operationalized as incidents per 1,000 population. 
 
Control Variables 
Percentage of planned polling centers closed. This measure is operationalized from NDI election 
data as the number of closed polling centers on election day divided by the total number of 
planned polling stations prior to election day, multiplied by 100. This measure differs from 
Weidmann and Callen (2013), who measured the number of closed polling stations. We deviate 
from this measure for two reasons. First, data on the number of planned polling stations is 
unreliably measured; we have more confidence in data on planned polling centers. Second, 
normalizing by the number of planned centers adjusts for underlying district population differences 
as reflected by number of polling centers. Weidmann and Callen (2013) did not normalize polling 
station counts because each station is limited to collecting 600 ballots, so normalization was 
inherent in the measure. This is not the case with polling centers, which can support variable 
numbers of polling stations. We therefore normalize the measure as a percentage to make the 
measure meaningful in context.  
 
Electrification. Weidman and Callen (2013) developed a measure of proportion of households 
with electricity from the 2007 National Risk and Vulnerability Assessment (NRVA) survey, 
administered by the Ministry for Rural Rehabilitation and Development (MRRD) and the Central 
Statistics Office (CSO) of Afghanistan, and based on information from N=20,576 households in 
383 districts. A similar measure of electrification will be created from the 2013-2014 Afghanistan 
Living Conditions Survey (ALCS), the successor to the NVRA, which was based on information 
from N=20,786 households in 364 districts. Although Weidmann and Callen (2013) do not 
explicitly describe how they operationalized this measure from NRVA questionnaire items, we will 
operationalize this measure as the survey weighted percentage of households within each district 
answering affirmatively to any response option for the following question: “Has your household 
had electricity at any time in the past month from any of these sources?: Electric grid, Government 
generator, Private generator (engine), Private generator (hydro), Community generator (engine), 
Community generator (hydro), Solar, Wind, Battery.” Our operationalization differs from Weidman 
and Callen (2013) in scaling: rather than a proportion (0-1) we express this measure as a 
percentage (0-100) to improve model interpretation.  
 

 
Per capita expenditure. Weidmann and Callen (2013) also developed a measure of per capita 
expenditure by households using the NRVA, but again were not explicit about how they 
operationalized this measure from NRVA questionnaire items. We will operationalize a weighted 
measure of past-month per capita expenditures (in Afghanis) using the 2013-14 ALCS. In 
particular, we will operationalize this measure as follows: 
1. Sum the reported past-month household expenditures (“How much did this household 
spend in the last month for each type of fuel used in the household?: Electricity; Gas; Fuel 
oil; Firewood; Ping, straw, manure”) and other household costs (“In the past month, how 
much did your household spend on the following categories of items? Food at home; Food 
away from home; Grooming, cosmetics, laundry soap, shampoo; Housing rent; 
Communication (phone, internet); Transportation of household members”). 
2. Sum the reported past-year household expenditures for other household expenses (“In 
the past year, how much did your household spend on the following categories of items?: 
Clothing and shoes; Health services and medicines; Education (tuition, books, uniform, 
supplies”), and then divide by 12 to obtain average monthly expenditures.  
3. Sum the past-month and average past-month household expenditure amounts from steps 
1 and 2. 
4. Sum the survey weighted total expenditure amounts by district. 
5. Divide by 1,000 to express expenditures in thousands of Afghanis. 
6. Divide by the district population to express as per capita household expenditures.  
 
Distance to Kabul. Data on the distance (km) from each district to Kabul is measured as the great 
circle distance on a spherical earth, as with the original study. This measure will be 
operationalized as the straight-line distance from each district center to the capital of Kabul based 
on latitude and longitude coordinates. In particular, we used the user-written Stata command -
geodist- to calculate these distances.  
 
Elevation. Average elevation data (in kilometers) for each district are based on World Bank staff 
estimates of 2015 satellite data from NASA’s Shuttle Radar Topography Mission (SRTM), 
supplemented for missing data in nine districts by using Google Earth queries by geographic 
coordinates. By comparison, Weidmann and Callen (2013) calculated elevation from US 
Geological Survey data. Our operationalization differs from Weidman and Callen (2013) in 
scaling: we express as kilometers rather than meters to improve model interpretation.  
  
Population. Population data used to normalize other measures are captured at the district level to 
a specificity in the thousands based on official government statistics.  
 
Clustering variable. Weidmann and Callen (2013) clustered their analyses by coalition forces 
regional commands (RCs). In 2009, the 34 provinces were clustered into one of four RCs. By 
2014, the number of RCs had expanded to six (see here). Our main clustering variable keys on 
these six RCs.  
 

 
18. Indices 
 
N/A 
Analysis Plan 
19. Statistical models 
 
For the purposes of SCORE, to test the combination of H1 and H* in line with the original paper 
(Table 2, Model 1), we will use logit models to regress the last-digit fraud measure on violence 
and its squared term to test the inverted U-shaped hypotheses, while controlling for covariates 
indicated above. The measure of violence in this model is the incidence rate per 1,000 population 
within a 5-day window around election day.  
 
We will also perform a supplemental analysis using the alternative measure of violence defined 
as the incidence rate per 1,000 population during a 60-day window preceding election day (Table 
2, Model 2).   
 
For both analyses, we will estimate cluster-robust standard errors by regional command, 
consistent with Weidmann and Callen (2013). We will also estimate supplemental models using 
multiway clustering grouped by regional command and election cycle to additionally account for 
pooling of election results.  
 
All analyses will be performed using Stata MP 16.1. Analytic code is provided on the OSF project 
page in a Stata do file (Weidmann_Data_Analysis.do). Test results of the planned analyses on a 
5% random sample of the data is provided on the OSF project page. The results demonstrate the 
analytic code works properly.  
 
20. Transformations 
 
We will use a quadratic (i.e., squared) term for the focal independent variable, violence, to test 
the nonlinear hypothesis that election fraud will increase as violence increases to a point, 
whereupon election fraud will decline at higher levels of violence.  
 
21. Inference criteria 
 
Criteria for a successful replication attempt for the SCORE project is a statistically significant 
effect (p < .05, two tailed) in the same pattern as the original study on the focal hypothesis tests 
(H1 and H*). For this study, this criteria is met by a positive and significant coefficient on the linear 

 
term for election violence (H1) and a negative and significant coefficient on the squared term for 
election violence (H*) in the focal regression model. 
 
22. Data exclusion 
 
We do not have any a priori exclusion criteria for observations in our sample.  
 
23. Missing data 
 
Consistent with Weidmann and Callen (2013), we will perform a complete case analysis. In the 
original study, this resulted in the loss of 6% of the data (375/398). In the current replication, a 
complete case analysis is expected to result in the loss of 76 observations, or less than 10% of 
the data (724/800), due to missingness on the ALCS measures.  
 
24. Exploratory analysis  
 
N/A 
 
25. Other 
 
The final aggregated data set will be made publically available on the OSF project page.  
 
References 
 
Bradley, Walker and Brian Blankespoor. (2019). Measuring Mobility in Afghanistan Using Travel 
Time Models Methodology Note. Washington, D.C.: World Bank Group. 
 
Weidmann, Nils B. and Michael Callen. (2013). Violence and Election Fraud: Evidence from 
Afghanistan. British Journal of Political Science, 43, 53-75.  

=== END OF REFERENCE JSON WITH THE CORRECT INFO ===

Please return your evaluation as a JSON object where each key is a specific component from the original JSON. For example:
{
    "hypothesis": {
    "score": [your scoring according to the instruction],
    "explanation": [reasoning for your scoring.]
    },
    "data_plan.source_type": {
    "score": [your scoring according to the instruction],
    "explanation": [reasoning for your scoring.]
    },
    ...
}
Output Requirements:\n- Return a valid JSON object only.\n- Do NOT wrap the output in markdown (no ```json).\n- Do NOT include extra text, commentary, or notes.\n\n Ensure accuracy and completeness.\n- Strictly use provided sources as specified.


