{
  "subsets": [
    {
      "subset": "gt2010",
      "n_used": 2269,
      "class_sizes": {
        "Traditional": 895,
        "Modern": 846,
        "Post-secular": 528
      },
      "profiles": [
        {
          "class_id": 0,
          "knowledge_mean": 0.728219696969697,
          "evolved_rate": 0.032196969696969696,
          "bible_literalist_rate": 0.4962121212121212,
          "n": 528,
          "label": "Post-secular",
          "subset": "gt2010"
        },
        {
          "class_id": 1,
          "knowledge_mean": 0.47108938547486034,
          "evolved_rate": 0.40893854748603353,
          "bible_literalist_rate": 0.4972067039106145,
          "n": 895,
          "label": "Traditional",
          "subset": "gt2010"
        },
        {
          "class_id": 2,
          "knowledge_mean": 0.8271276595744681,
          "evolved_rate": 0.8723404255319149,
          "bible_literalist_rate": 0.027186761229314422,
          "n": 846,
          "label": "Modern",
          "subset": "gt2010"
        }
      ],
      "welch_ttest": {
        "t_stat": -20.754845885162993,
        "p_value": 2.9368783173183073e-82,
        "mean_traditional": 0.40893854748603353,
        "mean_postsecular": 0.032196969696969696,
        "n_traditional": 895,
        "n_postsecular": 528,
        "se_diff": 0.01815198146368241,
        "ci_diff_95": [
          -0.41231946145788134,
          -0.3411636941202463
        ]
      },
      "two_prop_ztest": {
        "z_stat": -15.479553324607908,
        "p_value": 0.0,
        "p_traditional": 0.40893854748603353,
        "p_postsecular": 0.032196969696969696
      },
      "direction": "negative"
    },
    {
      "subset": "all",
      "n_used": 5260,
      "class_sizes": {
        "Traditional": 2169,
        "Modern": 1938,
        "Post-secular": 1153
      },
      "profiles": [
        {
          "class_id": 0,
          "knowledge_mean": 0.7756938421509106,
          "evolved_rate": 0.03555941023417172,
          "bible_literalist_rate": 0.46747614917606245,
          "n": 1153,
          "label": "Post-secular",
          "subset": "all"
        },
        {
          "class_id": 1,
          "knowledge_mean": 0.46536422314430614,
          "evolved_rate": 0.34485938220378054,
          "bible_literalist_rate": 0.4896265560165975,
          "n": 2169,
          "label": "Traditional",
          "subset": "all"
        },
        {
          "class_id": 2,
          "knowledge_mean": 0.8246904024767802,
          "evolved_rate": 0.8787409700722394,
          "bible_literalist_rate": 0.02889576883384933,
          "n": 1938,
          "label": "Modern",
          "subset": "all"
        }
      ],
      "welch_ttest": {
        "t_stat": -26.721261986166535,
        "p_value": 9.060239928184184e-142,
        "mean_traditional": 0.34485938220378054,
        "mean_postsecular": 0.03555941023417172,
        "n_traditional": 2169,
        "n_postsecular": 1153,
        "se_diff": 0.011575051063446473,
        "ci_diff_95": [
          -0.3319870720539639,
          -0.2866128718852537
        ]
      },
      "two_prop_ztest": {
        "z_stat": -19.94196858913803,
        "p_value": 0.0,
        "p_traditional": 0.34485938220378054,
        "p_postsecular": 0.03555941023417172
      },
      "direction": "negative"
    },
    {
      "subset": "le2010",
      "n_used": 2991,
      "class_sizes": {
        "Traditional": 1210,
        "Modern": 1124,
        "Post-secular": 657
      },
      "profiles": [
        {
          "class_id": 0,
          "knowledge_mean": 0.7918569254185692,
          "evolved_rate": 0.0258751902587519,
          "bible_literalist_rate": 0.4748858447488584,
          "n": 657,
          "label": "Post-secular",
          "subset": "le2010"
        },
        {
          "class_id": 1,
          "knowledge_mean": 0.45764462809917356,
          "evolved_rate": 0.3066115702479339,
          "bible_literalist_rate": 0.47851239669421486,
          "n": 1210,
          "label": "Traditional",
          "subset": "le2010"
        },
        {
          "class_id": 2,
          "knowledge_mean": 0.8203959074733096,
          "evolved_rate": 0.8745551601423488,
          "bible_literalist_rate": 0.03202846975088968,
          "n": 1124,
          "label": "Modern",
          "subset": "le2010"
        }
      ],
      "welch_ttest": {
        "t_stat": -19.178566884694565,
        "p_value": 3.62051608678614e-74,
        "mean_traditional": 0.3066115702479339,
        "mean_postsecular": 0.0258751902587519,
        "n_traditional": 1210,
        "n_postsecular": 657,
        "se_diff": 0.014638027005720816,
        "ci_diff_95": [
          -0.3094269129203948,
          -0.2520458470579692
        ]
      },
      "two_prop_ztest": {
        "z_stat": -14.277306228425454,
        "p_value": 0.0,
        "p_traditional": 0.3066115702479339,
        "p_postsecular": 0.0258751902587519
      },
      "direction": "negative"
    }
  ]
}