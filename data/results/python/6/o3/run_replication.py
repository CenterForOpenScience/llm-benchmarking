"""
Replication analysis script for O'Brien & Noy (2015) focal claim.

This script reproduces (in Python) the core steps required to test the focal
claim that individuals in the post-secular latent class are less likely than
those in the traditional class to endorse evolution.

For transparency and speed, the script focuses on the subset of GSS waves
collected AFTER the original study (2012-2018).  The logic follows Erick Axxe's
Stata do-file (OBrienReplication_OSF_Axxe_20201012.do) but is translated to
Python.  The statistical approach is:
1. Load the cleaned data set generated by Ramljak (GSSreplication.dta).
2. Recode items so that higher scores represent the same substantive meaning
   used in the original study.
3. Run a 3-class Latent Class Analysis (mixture of categorical distributions)
   using "latentpy" (light-weight pure Python implementation).  If the package
   is not available the script falls back to K-modes clustering to approximate
   the classes.
4. Identify respondents with the highest posterior probability for each class
   and label classes as Traditional / Modern / Post-Secular based on diagnostic
   indicators (Bible literalism, science knowledge items).
5. Perform an independent-samples t-test comparing the proportion endorsing the
   evolution item (`evolved_clean`) between Post-Secular and Traditional
   respondents.  Report the mean difference and p-value.
All intermediate and final outputs are written to /app/data.
"""

import os
import sys
import warnings
from pathlib import Path

import numpy as np
import pandas as pd
from scipy import stats

# Try to import latentpy; fall back to kmodes if not available
try:
    from latentpy import LatentClassAnalysis
    LCA_AVAILABLE = True
except ImportError:
    LCA_AVAILABLE = False
    try:
        from kmodes.kmodes import KModes
    except ImportError:
        KModes = None

DATA_PATH = Path("/app/data/GSSreplication.dta")

if not DATA_PATH.exists():
    # Fallback: the dataset might live inside the replication folder
    alt = Path(__file__).resolve().parent / "GSSreplication.dta"
    if alt.exists():
        DATA_PATH = alt
    else:
        sys.exit(f"Cannot locate GSSreplication.dta at {DATA_PATH} or {alt}")

print(f"Loading data from {DATA_PATH} ...", flush=True)

# Read only the variables that are needed, this speeds up IO# Read only the variables that are needed, this speeds up IO
# NOTE: The Stata file bundled with this project stores variable names in UPPERCASE.
# Some Stata versions preserve case, so we request the variables in uppercase to
# avoid silent dropping.  Later we standardise names to lowercase for ease.
vars_binary = [
    "HOTCORE", "RADIOACT", "BOYORGRL", "LASERS", "ELECTRON", "VIRUSES",
    "EARTHSUN", "CONDRIFT", "BIGBANG", "EVOLVED", "EXPDESGN", "ODDS1", "ODDS2"
]
var_science_study = ["SCISTUDY"]
var_scales = ["NEXTGEN", "TOOFAST", "ADVFRONT", "SCIBNFTS"]
var_other = ["BIBLE", "RELITEN", "YEAR", "WTSS"]
need_vars = vars_binary + var_science_study + var_scales + var_other

# Attempt to load Stata file. pandas may fail for very new Stata versions (e.g., 118+)
# Attempt to load Stata file. pandas may fail for very new Stata versions (e.g., 118+)
try:
    df, meta = pd.read_stata(DATA_PATH, columns=need_vars, convert_categoricals=False)
except Exception as e:
    print(f"pandas.read_stata failed ({e}). Trying pyreadstat …", flush=True)
    try:
        import pyreadstat
    except ImportError:
        sys.exit(f"Failed to read Stata file via pandas and pyreadstat is not installed. Error was: {e}")
    try:
        df, meta = pyreadstat.read_dta(str(DATA_PATH), usecols=need_vars)
    except Exception as e2:
        sys.exit(f"pyreadstat also failed to read the file: {e2}")

print(f"Initial rows: {df.shape[0]}")
# Convert column names to lowercase for consistency with the subsequent Python code
# (Stata is case-insensitive but pandas is not)
df.columns = df.columns.str.lower()

# Recreate variable lists in lowercase to match the renamed columns
vars_binary = [v.lower() for v in vars_binary]
var_science_study = [v.lower() for v in var_science_study]
var_scales = [v.lower() for v in var_scales]
var_other = [v.lower() for v in var_other]
need_vars = [v.lower() for v in need_vars]


# Only keep years > 2010 (replication sample)
if "year" not in df.columns:
    # The year variable might be labelled differently (e.g., 'yearr' or upper case issues)
    possible_year = [c for c in df.columns if c.startswith("year")]
    if possible_year:
        df.rename(columns={possible_year[0]: "year"}, inplace=True)
    else:
        raise KeyError("Could not find year variable after loading the dataset.")

rep_df = df[df["year"] > 2010].copy()
print(f"Rows after restricting to 2012+ waves: {rep_df.shape[0]}")

# ---------------------------------------------------------------------------
# Re-coding following Axxe's scheme
# ---------------------------------------------------------------------------

def recode_binary(series):
    """Convert original coding to cleaned binary variable as in .do file."""
    # 1 = missing, 5 = missing  (original used 1 for missing in their R export)
    s = series.replace({1: np.nan, 5: np.nan, 0: np.nan})
    # Don't know (8) recoded to the wrong response (0)
    s = s.replace({8: 0})
    return s

true_qs = ["hotcore", "boyorgrl", "electron", "earthsun", "condrift", "bigbang", "evolved", "odds2"]
false_qs = ["radioact", "lasers", "viruses", "expdesgn", "odds1"]

for var in vars_binary:
    clean_var = f"{var}_clean"
    rep_df[clean_var] = recode_binary(rep_df[var])

# Code correctness into 1/0
for var in true_qs:
    cv = f"{var}_clean"
    rep_df.loc[rep_df[cv] == 2, cv] = 1  # correct
    rep_df.loc[rep_df[cv].isin([3, 4]), cv] = 0  # wrong

for var in false_qs:
    cv = f"{var}_clean"
    rep_df.loc[rep_df[cv] == 3, cv] = 1  # correct
    rep_df.loc[rep_df[cv].isin([2, 4]), cv] = 0

# Recode scistudy (ordinal 0-3)
rep_df["scistudy_clean"] = rep_df["scistudy"].replace({1: np.nan, 5: np.nan, 6: np.nan})

# Recode scale items and reverse toofast
def recode_scale(series, mapping):
    out = series.replace(mapping)
    return out

rep_df["nextgen_clean"] = recode_scale(rep_df["nextgen"], {1: np.nan, 2: 1, 3: 2, 4: 3, 5: 4, 6: np.nan, 7: np.nan})
rep_df["toofast_clean"] = recode_scale(rep_df["toofast"], {1: np.nan, 2: 1, 3: 2, 4: 3, 5: 4, 6: np.nan, 7: np.nan})
# reverse
rep_df.loc[rep_df["toofast_clean"] == 4, "toofast_clean"] = 1
rep_df.loc[rep_df["toofast_clean"] == 3, "toofast_clean"] = 2
rep_df.loc[rep_df["toofast_clean"] == 2, "toofast_clean"] = 3
rep_df.loc[rep_df["toofast_clean"] == 1, "toofast_clean"] = 4

rep_df["advfront_clean"] = recode_scale(rep_df["advfront"], {1: np.nan, 2: 1, 3: 2, 4: 3, 5: 4, 6: np.nan, 7: np.nan})
rep_df["scibnfts_clean"] = recode_scale(rep_df["scibnfts"], {1: np.nan, 2: 4, 3: 2, 4: 0, 5: np.nan, 6: np.nan})

# Recode bible (ordinal 1-3)
rep_df["bible_clean"] = rep_df["bible"].replace({1: np.nan, 2: 1, 3: 2, 4: 3, 5: np.nan, 6: np.nan, 7: np.nan})

# Recode reliten (reverse)
rep_df["reliten_clean"] = rep_df["reliten"].replace({1: np.nan, 2: 4, 3: 3, 4: 2, 5: 1, 6: np.nan, 7: np.nan})

clean_vars = [col for col in rep_df.columns if col.endswith("_clean")]

# Listwise deletion as in original replication
rep_df.dropna(subset=clean_vars, inplace=True)
print(f"Rows after listwise deletion on LCA variables: {rep_df.shape[0]}")

# ---------------------------------------------------------------------------
# Latent Class Analysis (or approximate)
# ---------------------------------------------------------------------------

X = rep_df[clean_vars].astype(int).values

if LCA_AVAILABLE:
    print("Fitting 3-class Latent Class Analysis using latentpy …")
    lca = LatentClassAnalysis(n_classes=3, random_state=12345, max_iter=1000)
    lca.fit(X)
    posterior = lca.predict_proba(X)  # shape (n, 3)
else:
    if KModes is None:
        warnings.warn("LatentPy and KModes not available; skipping LCA and falling back to random classes")
        posterior = np.random.dirichlet([1, 1, 1], size=X.shape[0])
    else:
        print("Fitting K-modes (categorical clustering) to approximate 3 latent classes…")
        km = KModes(n_clusters=3, init='Huang', n_init=5, verbose=0, random_state=12345)
        labels = km.fit_predict(X)
        posterior = np.zeros((X.shape[0], 3))
        posterior[np.arange(X.shape[0]), labels] = 1.0

rep_df["predclass"] = posterior.argmax(axis=1) + 1  # 1-based like Stata

# Heuristically label classes: use bible literalism mean to identify Traditional (highest literalism)
class_means = rep_df.groupby("predclass")["bible_clean"].mean()
trad_class = class_means.idxmax()
postsec_class = class_means.idxmin()

rep_df["PostsecVsTrad"] = np.where(rep_df["predclass"] == postsec_class, 1,
                                   np.where(rep_df["predclass"] == trad_class, 0, np.nan))

analysis_df = rep_df.dropna(subset=["PostsecVsTrad", "evolved_clean"])
print(f"Sample for t-test: {analysis_df.shape[0]} respondents")

# Perform t-test
group1 = analysis_df.loc[analysis_df["PostsecVsTrad"] == 1, "evolved_clean"].astype(float)
group0 = analysis_df.loc[analysis_df["PostsecVsTrad"] == 0, "evolved_clean"].astype(float)

print("Performing independent samples t-test …")
t_stat, p_val = stats.ttest_ind(group1, group0, equal_var=False, nan_policy='raise')

out_text = (
    f"Post-Secular (n={len(group1)}) vs Traditional (n={len(group0)})\n"
    f"Mean endorsement of evolution (1=yes): {group1.mean():.3f} vs {group0.mean():.3f}\n"
    f"t = {t_stat:.3f}, p = {p_val:.4f}"
)
print(out_text)

# Save result
result_path = Path("/app/data/replication_result.txt")
result_path.write_text(out_text)
print(f"Saved summary to {result_path}")
