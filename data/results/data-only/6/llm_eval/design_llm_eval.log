=== GENERATED PROMPT ===

You are an information verifier.
You are given a json object and a reference document, your task is to score the information (key, value pair) presented in the extraced JSON object based on the information presented in the reference document.

=== START OF EXPLANATION OF WHAT EACH FIELD IN THE JSON MEAN ===
{
  "original_study": {
    "claim": {
      "hypotheses": "A testable hypothesis based on the claim",
      "hypotheses_location": "Specific location where the hypotheses is stated in the original paper",
      "statement": "The main claim made by the original study that matches the claim given as an input.",
      "statement_location": "Specific location where the claim is stated in the original paper.",
      "study_type": "Type of study (Experimental, Observational, Meta-Analysis)"
    },
    "data": {
      "source": "Data Source (e.g., survey, database)",
      "wave_or_subset": "Specific waves or subsets (if applicable)",
      "sample_size": "Sample size of the selected data (if applicable)",
      "unit_of_analysis": "Unit of analysis (e.g., individual, household)",
      "access_details": "Access details (e.g., restrictions, request process)",
      "notes": "Any additional information or caveats (e.g., encoding issues, nested structure, missing metadata, unusual column formats)"
    },
    "method": {
      "description": "Narrative summary of how the study was conducted.",
      "steps": "Ordered list of procedural steps to reproduce the study.",
      "models": "Models or statistical approach (e.g., regression type)",
      "outcome_variable": "Dependent/outcome variable measured or analyzed",
      "independent_variables": "Primary variables expected to influence the outcome",
      "control_variables": "Variables controlled for in the analysis",
      "tools_software": "Tools or software specifics (e.g., mentions of R, Python, packages)"
    },
    "results": {
      "summary": "Narrative summary of the main results or findings from the original study.",
      "numerical_results": [
        {
          "outcome_name": "Name or label of the outcome (e.g., 'test_score', 'conversion_rate')",
          "value": "Numeric result value (e.g., 0.45)",
          "unit": "Optional unit of measurement (e.g., %, points, ms)",
          "effect_size": "Optional effect size (e.g., Cohen's d, odds ratio)",
          "confidence_interval": {
            "lower": "Lower bound (e.g., 0.32)",
            "upper": "Upper bound (e.g., 0.58)",
            "level": "Confidence level, e.g., 95"
          },
          "p_value": "Optional p-value associated with the result (e.g., 0.001)",
          "statistical_significance": "Boolean indicating if result is statistically significant",
          "direction": "Qualitative direction of the effect (positive, negative, or null) based on the sign and magnitude of the reported result."
        }
      ]
    },
    "metadata": {
      "original_paper_id": "DOI or unique identifier.",
      "original_paper_title": "Title of the original paper.",
      "original_paper_code": "Link to the original study's codebase.",
      "original_paper_data": "Link to the original study's dataset(s)."
    }
  }
}
=== END OF EXPLANATION OF WHAT EACH FIELD IN THE JSON MEAN ===

Follow the rubrics below for your evaluation of each component in the JSON. The rubric uses a 0-3 scoring scale for all components, where:
- 3: Exact Match – The extracted information is identical or nearly identical to the reference in the document. The meaning is preserved completely, with the same level of detail, including all key elements (e.g., variables, relationships, or numerical values). No omissions or additions of unrelated information. 
- 2: Mostly Similar – The extracted information conveys the same core meaning as the reference, but with different phrasing or structure. Minor details may be omitted, but the essential information is preserved. If the extracted content is a finding that directly supports a hypothesis, consider it equivalent. 
- 1: Loosely Related – The extracted information has partial overlap with the reference but includes significant differences, omissions of major details, or additions of unrelated information. The core meaning is somewhat preserved but incomplete or altered.
- 0: No Match – No relevant overlap with the reference, completely incorrect, or missing entirely.
- NA: The reference document does not contain information to evaluate this component/field of the JSON.

=== START OF EXTRACTED JSON TO BE EVALUATED START ===
    {
  "replication_study": {
    "hypothesis": "Among GSS 2006/2008/2010 respondents classified into religio-scientific perspectives, the probability of endorsing that humans evolved from other animals is lower for the Post-secular class than for the Traditional class.",
    "study_type": "Observational",
    "data_plan": {
      "dataset_identifier": "GSSreplication.dta (GSS 2006, 2008, 2010 religio-scientific classes)",
      "source_type": "Survey (General Social Survey - GSS)",
      "wave_or_subset": "GSS 2006, 2008, and 2010 waves; modules on science attitudes/knowledge and religious items",
      "sample_size": "2901",
      "unit_of_analysis": "individual respondent",
      "access_details": "Provided locally in replication_data as GSSreplication.dta. At runtime, place the dataset at /app/data/GSSreplication.dta (or /app/data/GSSreplication.csv).",
      "qualification": {
        "explanation": "This dataset enables a direct replication of the focal claim because it appears to contain the same GSS waves and latent class membership needed for the test. Our Python script uses robust detection of the evolution item and class labels and reproduces the key comparison.",
        "similarity_to_original": "The original study uses the General Social Survey (GSS) 2006/2008/2010 waves with sample size 2,901 and latent classes Traditional, Modern, and Post-secular (post_registration.json; original_paper.pdf summary). The focal claim compares Post-secular vs Traditional on the evolution item (initial_details.txt). The replication dataset file GSSreplication.dta in replication_data is clearly aligned with this scope.",
        "deviation_from_original": "We do not re-estimate the latent classes via LCA; instead, we use the provided class membership presumed to be included in GSSreplication.dta (original paper used LCA selection via BIC/LMR, original_paper.pdf summary). The original paper notes survey design/missing data handling (post_registration.json). Our replication applies listwise deletion for the focal outcome and class, and includes an optional weighted analysis if a weight variable (e.g., wtssall) is available. This is a direct replication rather than a new sample; no additional GSS waves are added."
      },
      "notes": "Variable naming may differ from the original; the Python script detects evolution and class columns via name heuristics. Evolution responses are recoded to a binary variable (1 = endorses humans evolved; 0 otherwise), ignoring DK/NA. All files are read/written from /app/data. If the dataset is provided as CSV instead of DTA, the script will detect and load it. Weighted analysis is attempted if a common GSS weight (e.g., wtssall) is detected."
    },
    "planned_method": {
      "steps": [
        "Copy GSSreplication.dta (or GSSreplication.csv) into /app/data.",
        "Run the Python script replication_data/replicate_evolution_postsecular__py.py.",
        "Script loads /app/data/GSSreplication.(dta|csv) and detects the evolution item and class label column.",
        "Recode the evolution item to evol_binary (1 = endorses humans evolved from other animals; 0 otherwise).",
        "Normalize class labels to Traditional, Modern, Post-secular and subset to valid rows.",
        "Compute unweighted class-specific proportions and save to /app/data/replication_evolution_summary.csv.",
        "Estimate unweighted logistic regression evol_binary ~ C(class_norm) (Traditional as reference) and save results to /app/data/replication_evolution_results.json.",
        "If a survey weight is detected (e.g., wtssall), compute weighted class-specific proportions and fit a weighted GLM (Binomial) using freq_weights; append results to the same JSON."
      ],
      "models": "Unweighted binary logistic regression; Weighted GLM (Binomial) with frequency weights if a weight column is present. Descriptive difference in proportions (Post-secular vs Traditional).",
      "outcome_variable": "evol_binary: indicator for endorsement that humans evolved from other animals (1=yes, 0=no).",
      "independent_variables": "C(class_norm) with categories Traditional (reference), Modern, Post-secular.",
      "control_variables": "None (focal bivariate class comparison aligning with the claim).",
      "tools_software": "Python 3.10; pandas; statsmodels; numpy; pyreadstat (for .dta).",
      "planned_estimation_and_test": {
        "estimation": "Coefficient for Post-secular vs Traditional from the logit/GLM (log-odds) and class-specific proportions; optionally an odds ratio can be computed from the coefficient.",
        "test": "Wald z-test (two-tailed) on the Post-secular vs Traditional coefficient from the logistic/GLM model; assess p-value against alpha = 0.05."
      },
      "missing_data_handling": "Listwise deletion: exclude observations with missing/invalid responses for evol_binary or class_norm; ignore DK/refused for the evolution item.",
      "multiple_testing_policy": "Single focal outcome and contrast; no correction needed.",
      "inference_criteria": "Two-tailed alpha = 0.05. Expected direction: Post-secular < Traditional (negative coefficient). Consider both unweighted and weighted estimates; declare successful replication if the sign is negative and p < 0.05 in at least the primary (unweighted) model and the class-specific proportion for Post-secular is lower than Traditional; compare magnitude against the reported ~3% gap (initial_details.txt)."
    },
    "codebase": {
      "files": {
        "replicate_evolution_postsecular__py.py": "Entrypoint script (to run). Loads /app/data/GSSreplication.dta (or CSV), detects/recode the evolution item, normalizes class labels to Traditional/Modern/Post-secular, computes class-specific proportions, fits unweighted logistic regression evol_binary ~ C(class_norm) with Traditional as the reference, and saves outputs to /app/data (replication_evolution_summary.csv and replication_evolution_results.json). If a weight variable is detected (e.g., wtssall), also computes weighted proportions and a weighted GLM Binomial (freq_weights), appending to the JSON results."
      },
      "notes": "Ensure the dataset is at /app/data using one of the expected filenames (GSSreplication.dta or GSSreplication.csv). The code uses pandas.read_stata (requires pyreadstat) and statsmodels for inference. Outputs: replication_evolution_summary.csv (proportions) and replication_evolution_results.json (model coefficients, p-values, weighted results if available)."
    },
    "docker_specs": {
      "base_image": "python:3.10-slim",
      "packages": {
        "python": [
          "pip>=23.0.0",
          "setuptools>=68.0.0",
          "wheel>=0.40.0",
          "numpy>=1.23.0",
          "pandas>=1.5.0",
          "statsmodels>=0.13.0",
          "pyreadstat==1.2.7"
        ],
        "other": [
          "bash",
          "locales",
          "build-essential",
          "gcc",
          "g++",
          "make",
          "zlib1g-dev",
          "libbz2-dev"
        ]
      },
      "hardware": {
        "gpu_support": "false",
        "min_gpu_memory_gb": "0",
        "min_ram_gb": "2"
      },
      "volumes": [
        "./data:/app/data"
      ]
    },
    "analysis": {
      "instructions": [
        "Place the dataset file into the mounted volume at /app/data, using the name GSSreplication.dta (preferred) or GSSreplication.csv.",
        "Run: python replication_data/replicate_evolution_postsecular__py.py",
        "Check /app/data/replication_evolution_summary.csv for class-specific evolution endorsement proportions and /app/data/replication_evolution_results.json for model coefficients and p-values (and weighted results if a weight variable was detected).",
        "Interpret the coefficient for C(class_norm)[T.Post-secular] (vs Traditional): negative and p < 0.05 indicates that the Post-secular class is significantly less likely to endorse evolution than the Traditional class, consistent with the original claim."
      ],
      "comparison_metrics": [
        "Direction and significance of the Post-secular vs Traditional coefficient in the logistic/GLM model (expect negative, p < 0.05).",
        "Difference in class-specific proportions: Post-secular proportion endorsing evolution minus Traditional proportion (expect negative and around the reported magnitude of ~-0.03).",
        "Optional: Odds ratio for Post-secular vs Traditional derived from the logit coefficient (expect OR < 1)."
      ]
    }
  }
}
=== END OF EXTRACTED JSON TO BE EVALUATED END ===
    
=== START OF REFERENCE DOCUMENT WITH THE CORRECT INFO ===
     
Replication of a Research Claim from O’Brien & Noy (2015),  
from ​American Sociological Review​. 
 
Replication Team: Marco Ramljak and Erick Axxe 
 
Research Scientist: Melissa Kline 
 
Action Editor: John Lloyd 
 
Independent Reviewers 
(add name below when you initiate review, comment “DONE” on your name when you finish): 
 
Reviewer #1: [Jan Philipp Röer] 
 
Reviewer #2: Hansika Kapoor] 
 
Reviewer #3:Scott Braithwaite 
 
 
 
Review Period: September 14 - September 21 
 
View-only links to: ​Original Paper​, ​Replication Data​, ​Replication Analysis 
 
 
 
 
 
 
 
 
 
 
 
Privacy Statement: Other teams are making predictions about the outcomes of many different 
studies, not knowing which studies have been selected for replication. As a consequence, the 
success of this project requires full confidentiality of this peer review process. This includes 
privacy about which studies have been selected for replication and all aspects of the discussion 
about these replication designs. 
 
 
 

 
Instructions for Data Analysts 
 
The preregistration for this replication study was started by a separate team of researchers who were 
responsible for identifying data sources and constructing them into a replication dataset(s) for your use in 
the analysis. They have completed sections 1-13 of the preregistration below, and included additional 
materials in the OSF project that document how the dataset was constructed.  
 
In cases where all of the underlying data sources were able to be freely shared and posted, the 
constructed dataset(s) have been posted to the OSF as well, which you are free to use in designing the 
analysis plan (see below for details). In cases where some or all of the data sources could ​not​ be freely 
shared or posted, the replication dataset(s) are not provided on the OSF. Rather, you will need to follow 
the instructions and code to first reconstruct the datasets, and then proceed with your work. In such 
cases, the team responsible for creating the dataset(s) has provided summary statistics in the OSF that 
correspond to the constructed datasets, so you can verify that the datasets you create match what they 
intended. 
 
You’ll be responsible for filling out sections 16-25 of the preregistration below. Before you do so, ​please 
review the original study, sections 1-15 of the preregistration, and the materials provided on the 
OSF​, so that you are familiar with all of the decisions that have been made to date. In many cases, the 
‘data preparer’ will have left you instructions and suggestions on how the provided data can be used in 
the analysis, as well as idiosyncrasies and discrepancies in the data that you should be aware of. The 
data preparers have tried to be thorough in including all variables that you might need, but please keep in 
mind the following: 
●
Some of the variables included in the constructed dataset(s) may not be needed in the final 
analysis, so please do not feel the need to necessarily use all of the provided variables. 
●
Some of the variables needed might have mistakenly been excluded from the constructed 
datasets. If you find that this is the case, please let ​Andrew​ or ​Anna​ know, and they will work with 
you to supplement the datasets as needed. 
 
For these secondary data replications, we would like the analysis plan to be completed before the 
preregistration goes through review, so that after review, the only remaining steps are registration and 
running the analysis code on the full datasets. To facilitate that, we are asking that you include in section 
19 a link to the code you will use that takes the constructed dataset(s) provided to you and produces the 
focal analysis (including all of the cleaning, merging, and transforming required). When developing your 
analysis plan and code, please randomly sample 5% of the data for use in your work, and ​do not use the 
rest of the data until after your study is registered and it is time to run the final analysis​. In section 
19, you will find a statement that we are asking you to bold that confirms you’ve only used 5% of the data 
when developing and testing your code. If this approach will not work for any reason, please let ​Andrew​ or 
Anna​ know and disclose deviations from this plan somewhere in the preregistration. 
●
In cases where we are providing you a complete dataset, you can just sample out 5% of the 
observations and hold the rest out until you are ready to perform the final analysis.  
●
In cases where we are providing you multiple datasets that need to be combined prior to analysis, 
please sample out 5% of the observations in whatever way is most sensible.  
○
For example, in cases where each dataset contains complete observations on its own (a 
typical 'row bind' situation), it makes the most sense to sample out 5% of each dataset 
separately and then combine them together to develop and test your code.  
○
In cases where datasets need to be merged in order to create complete observations (a 
typical 'column bind' situation), it makes the most sense to merge the separate datasets 
 

 
into a full dataset first, and then sample out the 5% before proceeding with the rest of the 
analysis code. 
●
We leave the decision on how to sample out the random subset of data to you, so long as (a) you 
are not performing any analyses on the complete dataset until after your study is registered and 
(b) whatever decision you make is documented in the preregistration. 
 
Finally, in cases where the replication data combines observations from the original study with 
observations that were not used in the original study (what we are calling ‘hybrid replications’), please 
perform up to three analyses (details immediately below). This will likely require you to subset your data, 
based on the description of the original analysis provided in the study. 
●
When the ‘new’ data alone can clear the minimum power threshold, please perform three 
analyses: one analysis that only uses observations that were ​not​ used in the original analysis; 
one analysis that combines all available observations; and a third analysis that only uses 
observations that ​were​ used in the original analysis. Please make sure all three analyses are 
documented (with code) in section 19 below. 
●
When the ‘new’ data alone ​cannot​ clear the minimum power threshold, please perform one 
analysis that combines all available data, and a second that only uses the old data. Please make 
sure both analyses are documented (with code) in section 19 below. 
 
Please contact ​Andrew​ or ​Anna​ if you have any questions. After you’ve completed the remaining 
sections of the preregistration and uploaded all the necessary materials to the OSF, please 
contact ​the SCORE coordinators​ regarding next steps. 
 
 
 

 
Preregistration of O’Brien_AmSocioRev_2015_7X54 
Existing Data Replication 
Study Information 
1. Title (provided by SCORE) 
RR TEAM INSTRUCTIONS: ​This has been determined by SCORE​. 
 
 
Replication of a research claim from O’Brien & Noy (2015) in ​American Sociological Review​. 
2. Authors and affiliations  
RR TEAM INSTRUCTIONS: ​Fill in the names and affiliations of your team below​. 
 
Marco Ramljak​2 
Erick Axxe​1 
 
The Ohio State University 1 
Utrecht University 2 
 
3. Description of study (provided by SCORE) 
RR TEAM INSTRUCTIONS: ​This description has been provided by SCORE. Please review and 
make a SCORE project coordinator aware of any edits, additions, and corrections you would 
suggest to the paragraph. You are free to add additional descriptions of your project in a 
separate paragraph.  
 
The claim selected from O’Brien & Noy (2015) is that, although the post-secular perspective 
entails high levels of science knowledge as well as favorable views of science and religion, 
when scientific and religious perspectives conflict (e.g. evolution), the post-secular latent class 
almost unanimously aligned their views with particular religious accounts. This reflects the 
following statement from the paper's abstract: "Overall, most individuals favor either scientific or 
religious ways of understanding, but many scientifically inclined individuals prefer certain 
religious accounts." Participants’ responses to the General Social Survey (GSS) were submitted 
to a latent class analysis that resulted in a three-class solution characterized as representing 
traditional, modern, and post-secular perspectives on science and religion. Following this 
assignment, two-tailed t-tests were used to compare responses between the three groups; for 
the purposes of the SCORE project, the focal test is the comparison between the Traditional 
and Post-Secular groups on the question concerning evolution (‘Human beings developed from 
 

 
earlier species of animals’, yes or no). Members of the post-secular category were significantly 
less likely than members of the traditional group to respond that humans evolved from other 
animals (3 percent, significant at p < 0.05 on a two-tailed test, see Table 2, rightmost column). 
 
4. Hypotheses (provided by SCORE with possible Data Analyst additions) 
RR TEAM INSTRUCTIONS:​ ​The focal test for SCORE is indicated as H*. If you will test 
additional hypotheses (or use alternate analyses) that help you to evaluate the claim your 
replication/reproduction is testing, number them H1, H2, H3 etc. (You can place H* in the list 
wherever makes sense). Please make sure that any additional hypotheses are logical 
deductions/operationalizations of the selected SCORE claim or are necessary to properly 
interpret the focal H* hypothesis.  Research that is outside this scope should be described in a 
separate preregistration. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Are the listed hypotheses specific, concise, clearly testable, and specified at the level of 
operationalized variables?  
●
Are hypotheses identified as directional or non-directional, and, if applicable, have the 
direction of hypotheses been stated? (Example: “Customers’ mean choice satisfaction 
will be​ ​higher in the CvSS architecture condition than in the standard attribute-by- 
attribute architecture condition.”) 
●
Does the list of hypotheses/tests indicate whether additional hypotheses are taken from 
the original study or modified/added by the team? 
 
 
H*:​ Respondents with a post-secular perspective on science and religion will be less likely than 
respondents with a traditional perspective on science and religion to respond that humans 
evolved from other animals. 
 
 
 
 

 
Design Plan 
5. Study type 
NOTE:​ ​The study type selected should be based on the data collected for the replication, and 
not necessarily the data used in the original study. 
 
●
Experiment - A researcher randomly assigns treatments to study subjects, this includes 
field or lab experiments. This is also known as an intervention experiment and includes 
randomized controlled trials. 
●
Observational Study - Data is collected from study subjects that are not randomly 
assigned to a treatment. This includes surveys, natural experiments, and 
regression discontinuity designs​. 
●
Meta-Analysis - A systematic review of published studies. 
●
Other  
6. Blinding 
RR TEAM INSTRUCTIONS:​ ​Select any/all of the below that apply for your study by bolding 
them. You will give a longer description in the next question. 
 
●
No blinding is involved in this study. 
●
For studies that involve human subjects, they will not know the treatment group to which 
they have been assigned. 
●
Personnel who interact directly with the study subjects (either human or non-human 
subjects) will not be aware of the assigned treatments. (Commonly known as “double 
blind”) 
●
Personnel who analyze the data collected from the study are not aware of the treatment 
applied to any given group. 
 
[QUESTION 6 - BOLD YOUR RESPONSE ABOVE] 
 
7. Blinding 
RR TEAM INSTRUCTIONS:​ ​Since all existing data replications are based on data that has 
already been collected, in most cases it will not be necessary to comment on participant 
blinding. In the rare instance when an existing experiment is being re-analyzed for an existing 
data replication and blinding is a relevant consideration, please provide below any details 
regarding blinding that are important for a reviewer to be aware of. 
 
No blinding was involved to the secondary data collectors’ knowledge. 
 

 
8. Study Design 
RR TEAM INSTRUCTIONS:​ ​Please describe how data was collected in the original study and 
how it compares to the data that was selected for the replication attempt. Explain why the data 
selected for the replication study is suitable for a replication and if any substantial deviations 
exist between the two. 
 
If the data used in the replication combines observations from the original study with new 
observations (e.g. if the data selected for the replication attempt comes from the same 
longitudinal survey as the original study), describe how ‘original’ and ‘new’ observations relate to 
each other and an estimate for what proportion of the final dataset’s observations will be 
comprised of original vs. new observations. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration specify the unit of analysis? 
●
Does the preregistration provide sufficient detail about how the data selected for the 
replication attempt deviates from or is congruent with the data employed in the original 
study? 
●
Does the preregistration describe whether and how ‘original’ and ‘new observations’ are 
combined together for the replication dataset? 
 
 
The original study utilizes standardized, representative data from the General Social Survey 
(GSS) - one of the largest biennial public opinion surveys in the USA. It inhibits core modules 
(questions asked in every wave) and special modules (topical questions asked only in certain 
waves). The original study uses the waves from 2006, 2008 and 2010.  
 
All questions needed to replicate the focal claim are entailed in core modules, therefore 
available for every forthcoming wave, from the point of view of the original study. Therefore, the 
forthcoming waves (2012, 2014, 2016, 2018) can all be utilized for potential replication. The 
replication can either be carried out only for the forthcoming waves or with a combined dataset 
(2006, 2008, 2010, 2012, 2014, 2016, 2018) as the questions are standardized. It can be 
concluded that there are no serious deviations present in the potential replication dataset 
compared to the dataset from the original study, concerning the relevant variables needed for 
the focal claim. 
 
It should be noted that the original study conducts a latent class analysis in which a 3 level 
latent variable is developed by “clustering” certain variables. This latent variable is used to 
differentiate three classes concerning the variables that are used for clustering. The focal claim 
is only interested in one of these variables and its distribution concerning the levels of the latent 
variable. Therefore, even though at the end the focal claim is only interested in one of the 
 

 
variables, to replicate the analysis to a sufficient degree, all so called manifest variables are still 
needed. Therefore, all manifest variables will be listed in the following. 
 
9. Randomization (free response) 
 
RR TEAM INSTRUCTIONS:​ ​If the variables used for this replication attempt were randomized, 
state how they were randomized, and at what level. 
 
No variables for this study have been randomly assigned. 
 
Sampling Plan 
 
This section describes how the data sources for the replication were selected, how they were 
prepared into a replication dataset, and the number of observations that will be analyzed from 
these data. Please keep in mind that the data described in this section are the actual data used 
for analysis, so if you are using a subset of a larger dataset, please describe the subset that will 
actually be used in your study. 
10. Existing data (multiple choice question, provided by SCORE) 
1.1.1.
Registration prior to creation of data 
1.1.2.
Registration prior to any human observation of the data 
1.1.3.
Registration prior to accessing the data 
1.1.4.
Registration prior to analysis of the data 
1.1.5.
Registration following analysis of the data 
 
11. Explanation of existing data 
NOTE:​ ​For replications that rely on existing data sources, this question refers to the data that 
will be used for the replication analysis (i.e. the final replication dataset), and not (a) the data 
from the original study or (b) the data sources accessed to construct the replication dataset. 
Since no new data will be created for ‘existing data replications,’ 1.1.1 should never be selected. 
Since all analyses will occur after registration, 1.1.5 should also never be selected. 
 
The final dataset for replication has been accessed, and cleaned to a very low degree prior to 
registration. All subsequent waves of the GSS (reference point: the waves used in the original 
dataset) were chosen because the necessary variables were available in all of them. The 
selected variables in the final dataset are the same ones that were used in the original study. 
 

 
None of the variables were selected because of their likelihood (or not) of leading to a 
confirmatory result.  
12. Data collection procedures 
RR TEAM INSTRUCTIONS:​ ​Please describe the process for constructing the replication 
dataset in as much detail as you can. The sections below should be used to provide the 
following information: 
●
Which variables are needed from the original study to perform a good-faith, high-quality 
replication.  
●
Which data sources were used, why they were selected, any deviations between the 
original study design and the replication study design that these selections present, and 
the procedures used to access the data. 
●
Which of the variables from the original study are available in the replication data 
sources, including relevant details about each measure. 
●
The procedure for creating the replication dataset, in both narrative and script form. 
●
A data dictionary that documents each variable included in the replication dataset. 
 
In the sections below, please provide links to the original materials whenever possible -- 
including descriptions of the original datasets and corresponding codebooks. If materials can be 
shared on the OSF, please do so, and provide view-only links to those materials. 
 
Specific points to keep in mind for reviewers: 
●
Does the preregistration describe which data sources were selected for the replication 
study and why each is suitable? 
●
Does the preregistration make clear how the data sources were used to construct the 
replication dataset? 
(a) Data Needed 
RR TEAM INSTRUCTIONS:​ ​List below the datasets and variables the original author used to 
analyze the focal claim. Include details regarding the sample size, waves or years used, and 
other details pertinent to finding an existing dataset for replication. Please include page 
numbers when excerpting from the original article. If possible, categorize the list of variables as 
one of the following: dependent variable, focal independent variable, control variable, or sample 
parameters/clustering variable. Finally, include the sample size of the original study’s focal 
analysis, if it is available. 
 
The variables needed for replication are so-called manifest variables. A list can be found in 
Table 1 of the online supplementary 
(​https://journals.sagepub.com/doi/suppl/10.1177/0003122414558919/suppl_file/obrien_online_s
upplement.pdf​) or here: ​https://osf.io/27e8c/​. They are categorized into three groups: fourteen 
Science Knowledge variables, four Science Attitudes variables, four Religion Indicator variables. 
All variables are derived from the same data source of the same waves, therefore the 
 

 
corresponding metadata is the same for all listed variables. The corresponding variable 
acronym used in the GSS, which is the same  for every wave is indicated in brackets after the 
variable wording and levels. The focal manifest variable is highlighted in yellow. 
 
Metadata on all manifest variables for the Latent Class Analysis  
●
Variable Data Source: General Social Survey (GSS) 
●
Waves / Years: 2006, 2008, 2010 
●
The original study is interested in the complete US population and mentions the data 
used are representative for the US population. Therefore, it is assumed the listed 
variables are used in combination with the available survey weights and appropriate 
survey techniques. 
●
In the original study, even though not indicated, they differentiate between informative 
and uninformative missing values. An example for informative missing values is, if an 
observation for example indicated “I don't know” to a knowledge question which can be 
recoded to a false answer. An example an uninformative missing value is, if an 
observation indicated “I don’t know” for an opinion question which cannot be 
meaningfully recoded. Therefore, when executing listwise deletion, which is indicated on 
page 97 in the original study, informative missing values were retained in the 
analysis-ready-sample while observations with at least one uninformative missing value 
for any of the listed variables were discarded from the analysis ready sample. Indicating 
which values for which variable is recoded as uninformative or informative missing is not 
indicated in the study however can be logically deduced. The variable list below entails 
only analysis-ready, meaningful levels (values). 
●
The sample size of the original study has 2901 observations (analysis ready 
observations). 
 
Manifest Variables group 1: Science Knowledge (1 = correct answer, 0 = wrong answer): 
●
Center of Earth very hot? (acronym: hotcore) 
●
All radioactivity man-made? (acronym: radioact) 
●
Father’s gene decides boy or girl? (acronym: boyorgrl) 
●
Lasers work by focusing sound waves? (acronym: lasers) 
●
Electrons smaller than atoms? (acronym: electron) 
●
Antibiotics kill viruses as well as bacteria? (acronym: viruses) 
●
Sun goes around Earth or Earth around Sun? (acronym: earthsun) 
●
Continents been moving for millions of years and will move in future? (acronym: condrift) 
●
Universe began with huge explosion? (acronym: bigbang) 
●
Human beings developed from earlier species of animals? (acronym: evolved) 
●
Understand experimental research design? (acronym: expdesign) 
●
Does one in four chance of inherited illness mean that if the first child has the illness, the 
next three will not? (acronym: odds1) 
●
Does one in four chance of inherited illness mean that each child has the same risk of 
having the illness? (acronym: odds2) 
 

 
●
Clear understanding of what it means to study something scientifically? (acronym: 
scistudy) 
 
Manifest Variables group 2: Science Attitudes 
●
Science and technology create more opportunities for the next generation (1=strongly 
disagree, 4=strongly agree) (acronym: nextgen) 
●
Science makes our way of life change too fast (1=strong agree, 4=strong disagree) 
(acronym: toofast) 
●
Scientific research that advances the frontiers of knowledge is necessary; should be 
supported by federal government (1=too little - 3 = too much) (acronym: natsci) 
●
Benefits of scientific research outweigh harm (0=harm strong outweighs benefits, 
4=benefits strong outweigh harm)? (acronym: scibnfts) 
 
Manifest Variables group 3: Religion Indicators 
●
Feelings about the bible (Word of God (1), Inspired Word (2), Book of Fables (3) 
(acronym: BIBLE) 
●
Strength of religious affiliation (1=none, 4=very strong) (acronym: reliten) 
 
(b) Data Access 
RR TEAM INSTRUCTIONS:​  ​Describe below the data sources that will provide the replication 
variables. Include information such as the name of the data source (e.g., Indonesian Family Life 
Survey), the description and link of the data source, and the waves needed to create a final 
replication dataset.  
 
Also describe the process for accessing the data sources that will be used to create the final 
replication dataset; specify how long long it took for the registration to be approved and what 
information was required (e.g., writeup of the purpose of the project, email address from an 
IPCSR institution, etc.); and verify that the data can be opened as expected. If applicable, 
provide a link to the page where you registered to access the data. 
 
Describe in detail any restrictions on data access and data-sharing, as well as any additional 
terms of data use that will be relevant for the replication study and final report (e.g. citations that 
will need to be made). If you were able to access the data because of special permissions that 
you have, but that you expect other researchers might not have, please document those as well. 
 
The data source for replication is the same as in the original study, however different waves will 
be utilized. The GSS is freely available for research purposes and offers a very user-friendly 
platform. A  free registration can be done to extract specific variables and waves in a 
machine-readable format (Stata and SPSS are available).  This registration process including 
activation and approval of the account is done within minutes. Usual individual information is 
asked e.g. name, address, institution, which can be filled in however only a valid email address 
 

 
and a password are necessary to complete registration. However, a cumulative data file with an 
accompanying codebook of all conducted waves can be downloaded without registration (65 mb 
zipped; 450 mb unzipped, 
https://gssdataexplorer.norc.org/pages/show?page=gss%2Fgss_data​). This is also the 
proposed approach for the construction of the replication dataset. The data file and the 
codebook can be successfully opened. 
 
A preprocessing code R-notebook is being delivered by the data finder and uploaded to OSF. 
This R-notebook entails all necessary code to construct the replication dataset, ready for 
analysis, in R. ​The final preprocessed data file can be found here: ​https://osf.io/aqmpd/​.  
 
Analyses done on GSS can be done freely for any kind of institution and research. For citations 
in academic journals the following should be used:  
“Smith, Tom W., Davern, Michael, Freese, Jeremy, and Morgan, Stephen, General Social 
Surveys, 1972-2018 [machine-readable data file] /Principal Investigator, Smith, Tom W.; 
Co-Principal Investigators, Michael Davern, Jeremy Freese, and Stephen Morgan; Sponsored 
by National Science Foundation. --NORC ed.-- Chicago: NORC, 2018: NORC at the University 
of Chicago [producer and distributor]. Data accessed from the GSS Data Explorer website at 
gssdataexplorer.norc.org​.“ 
 
For other kind of citing such as in media outlets the following short description is helpful: 
https://gssdataexplorer.norc.org/pages/show?page=gss%2Fcite. 
 
(c) Variable Availability 
RR TEAM INSTRUCTIONS: ​For each variable required for the replication analysis (listed 
above), describe the variables from the replication data that can be used to measure it 
(including which data files or sources each measure is found in), ​any notes a data analyst 
should consider when using the measure in a replication analysis​, and any important 
differences between the original variable and the proposed replication variable. 
 
If there are multiple variables in the replication data that correspond to a required variable (e.g. 
two different measures of education in the replication data), include all of those options below. If 
a variable from the original study ​cannot​ be measured using the replication data, please make 
that clear as well. ​Finally, include a description of the identifiers used to merge multiple 
datasets, if applicable. 
 
Manifest Variables group 1: Science Knowledge: 
●
All of the listed variables are the same as above and contain the following units (values) 
in this group (if not indicated otherwise below the variable):  
○
True (1), False (2), Don’t know (8), No answer (9), Not applicable (0) 
 

 
●
The values indicate the answer of the respondent not the actual “true” or “false” answer 
to the knowledge question. Not in the original study nor in the codebook of the GSS it is 
indicated which of the answers is the “correct” one.  
●
The list of the respective variables with acronyms in brackets: 
○
Center of Earth very hot? (acronym: HOTCORE) 
○
All radioactivity man-made? (acronym: RADIOACT) 
○
Father’s gene decides boy or girl? (acronym: BOYORGRL) 
○
Lasers work by focusing sound waves? (acronym: LASERS) 
○
Electrons smaller than atoms? (acronym: ELECTRON) 
○
Antibiotics kill viruses as well as bacteria? (acronym: VIRUSES) 
○
Sun goes around Earth or Earth around Sun? (acronym: EARTHSUN) 
○
Continents been moving for millions of years and will move in future? (acronym: 
CONDRIFT) 
○
Universe began with huge explosion? (acronym: BIGBANG) 
○
Human beings developed from earlier species of animals? (acronym: EVOLVED) 
○
Understand experimental research design? “Better way to test drug btw control 
and non-control” (acronym: EXPDESGN) (All 1000 get the drug (1), 500 get the 
drug 500 don’t (2), Don’t know (8), No answer (9), Not applicable (0)) 
○
Does one in four chance of inherited illness mean that if the first child has the 
illness, the next three will not? (acronym: ODDS1) (Yes (1), No(2), Don’t know 
(8), No answer (9), Not applicable (0)) 
○
Does one in four chance of inherited illness mean that each child has the same 
risk of having the illness? (acronym: ODDS2) (Yes (1), No (2), Don’t know (8), No 
answer (9), Not applicable (0)) 
○
Clear understanding of what it means to study something scientifically? 
(acronym: SCISTUDY) (Clear Understanding (1) - Little Understanding (3), Don’t 
know (8), No answer (9), Not applicable (0)) 
 
Manifest Variables group 2: Science Attitudes 
●
All of the listed variables are the same as above and contain the following units (values) 
in this group (if different units/scale is used it is indicated behind the acronym):  
○
Strongly Disagree (1) - strongly agree (4), Don’t know (8), No answer (9), Not 
applicable (0) 
●
The list of the respective variables with acronyms in brackets: 
○
Science and technology create more opportunities for the next generation 
(acronym: NEXTGEN) 
○
Science makes our way of life change too fast (acronym: TOOFAST) 
○
Scientific research that advances the frontiers of knowledge is necessary; should 
be supported by federal government (acronym: NATSCI) (Too little (1) - Too 
much (3), Don’t know (8), No answer (9), Not applicable (0) 
○
Benefits of scientific research outweigh harm? (acronym: SCIBNFTS) (Benefits 
greater (1) - Harmful results greater (3), Don’t know (8), No answer (9), Not 
applicable (0) 
 

 
 
Manifest Variables group 3: Religion Indicators 
●
All of the listed variables are the same as above. 
●
The list of the respective variables with acronyms in brackets: 
○
Feelings about the bible (acronym: BIBLE) (Word of God (1), Inspired Word (2), 
Book of Fables (3), Other (4), Don’t know (8), No answer (9), Not applicable (0) 
○
Strength of religious affiliation (acronym: RELITEN) (Strong (1) - no religion (4), 
Don’t know (8), No answer (9), Not applicable (0))  
 
It should be noted again that there is no deviation in any kind between the 
operationalization of the variable constructs of the original study and the one of the 
proposed replication as all variables (constructs) are derived from the same survey. The 
only deviation possible is the operationalization of informative and uninformative 
missings as this is not explicitly stated in the original study, however the estimated 
impact of this is rated extremely low (by the data finder) as only a handful of 
observations chose these categories (values).  
(d) Data Creation 
RR TEAM INSTRUCTIONS:​ ​Create a dataset using the data sources and variables listed 
above. Provide a detailed narrative describing how the various datasets were cleaned and 
merged into a final replication dataset. Provide a view-only link to a clearly commented script on 
the OSF that produces the replication data as described in the narrative. Our preference is that 
this be either an R script or a script from another language that similarly allows for open and 
reproducible analyses. Please let the SCORE team know if this is not possible. 
●
If the data can be freely shared and posted to OSF, please post it in your OSF project 
and provide a link to the completed dataset below. 
●
If any part of the dataset cannot be shared between researchers or posted to the OSF, 
please leave the final dataset off the OSF. Instead, include either below or in your script 
(commented out at the bottom) two pieces of information that will help an independent 
team verify they have created the dataset according to your instructions: 
○
The dimensions of the final dataset(s) you’ve created (# of rows, # of columns) 
○
A summary of 8-10 variables in the replication dataset. For numeric variables, the 
summary should include the mean, standard deviation, and count of NAs. For 
categorical variables, the summary should include each level present in the data 
and its count, as well as a count of NAs. If multiple datasets are submitted as part 
of your work, at least one variable should be included from each dataset. 
 
The data from the replication sources should be preserved in as ‘raw’ a form as possible, in 
order to give the data analyst the most latitude to clean the variables as they see fit. Variables 
from the original source should be preserved in their original form (e.g. do not recode values of 
99 to NA). New variables should only be created when they’re needed to complete the merge or 
 

 
combine the datasets; in those cases, please preserve a version of the original, unaltered 
variable in the new dataset.  
 
When combining multiple datasets by binding rows, please be sure that the data type and 
measurement units are equivalent across each dataset. If there is a discrepancy in how a 
variable is measured across datasets, rename the variable in each dataset to indicate the 
original dataset, and then carefully document the resulting measures below and in the data 
dictionary. ​See here for an example​ of how this should work. 
 
Please also use this section to describe: 
●
Any deviations between the original study design and the replication design that would 
result from using this replication dataset. 
●
Any notes about using these variables that you would like to pass along to the data 
analyst. 
 
The attached html R-notebook script is very detailed in its sections and can be found here: 
https://osf.io/wg8td/​. The uncompiled script version can be found here: ​https://osf.io/rzyx9/​. The 
most important thing the data analyst needs to understand is the use of informative and 
uninformative missings (indicated in section 12b) and the existence of knowledge questions 
which were answered by the respondents however no “correct” answer is provided by the GSS. 
The latter is important to know as the informative missing values of, specific for each variable, 
need to be recoded to the officially “wrong” category in order to preserve as many observations 
as possible. This is at least the authors’ way of doing of the original study. 
 
In the code file helper objects were built that indicate a sophisticated guess by the data finder, 
which values for which variable can be indicated as an informative missing value and therefore 
observations with these values should be preserved. 
 
(e) Data Dictionary 
RR TEAM INSTRUCTIONS​: ​Create ​a data dictionary​ following ​this template​. Provide below a 
view-only link to the completed data dictionary included in the OSF project. If the Data Analyst 
will need to create new variables using the variables in the final replication dataset (e.g. 
recoding the provided education variable to be in a better format for analysis), please document 
below your recommendation on how the analyst should do so. Please also document any 
additional notes regarding the variables in the dataset that do not fit within the provided data 
dictionary template or the other sections above. 
 
The data dictionary can be found here: ​https://osf.io/xknmw/​.  
 
The complete codebook of the GSS is uploaded here: ​https://osf.io/px65a/​.  
 
 

 
The data file can be found here: ​https://osf.io/aqmpd/ 
 
The R code for creating the dataset can be found here: ​https://osf.io/rzyx9/ 
 
13. Sample size 
RR TEAM INSTRUCTIONS​: ​Please report below the analytic sample size(s) in the replication 
dataset, with reference to however many units or levels are in the data. Please report as much 
information here as will be helpful for the review committee to be aware of, including differences 
in sample size resulting from various analytic decisions (e.g. listwise deletion vs multiple 
imputation). ​Finally, when ​the replication combines observations from the original study 
with new observations​, please ​estimate what proportion of the analytic sample’s 
observations will be comprised of original vs. new observations. 
 
Data finders’ response goes here: 
The proposed replication dataset includes four descending waves of the same data source, the 
GSS. Therefore the sample parameters are the same and can be perfectly replicated. The 
replication dataset of the proposed waves (2012, 2014, 2016, 2018) would entail 2164 
observations when doing listwise deletion (depending on the operationalization and inclusion of 
uninformative and informative missings, as approximately done as in the original study). The 
original study presents 2901 valid observations. In my opinion multiple imputation is not helpful 
here, as the questions are multifaceted and there is not always enough information available. 
 
------ 
 
Required sample size [to be filled out by the SCORE team]: The primary unit of analysis is the 
individual. An estimate of the minimum viable sample size for the data analytic replication is: 33. 
For comparison, the stage1 required sample size would be: 153 and the stage2 sample size 
would be: 339. 
 
Notes:​ Right now, the power analysis assumes that the proportion of the sample that falls into 
the traditional and postsecular groups will be the same in the replication as in the original (~66% 
vs. ~33%). The analysis assumes that the LCA that is used to produce the groups that are then 
used in the t-test will find the same obvious groups in the new data. 
 
14. Sample size rationale 
 
For data analytic replications in SCORE, three sample sizes are calculated: 
●
A minimum threshold sample size, defined as the sample size required for 50% power of 
100% of the original effect 
 

 
●
A stage 1 sample size, defined as the sample size needed to have 90% power to detect 
75% of the original effect 
●
A stage 2 sample size, defined as the sample size needed to have 90% power to detect 
50% of the original effect 
Details about how those sample sizes were calculated for this project ​are found here​. 
15. Stopping rule (provided by SCORE) 
RR TEAM INSTRUCTIONS:  
 
Since the replication dataset combines observations from the original study with a more recent 
set of observations, SCORE recommends that three analyses be performed: 
●
One analysis that only uses observations that were ​not​ used in the original study. 
●
One analysis that combines all available observations. 
●
One analysis that only uses observations that ​were​ ​used in the original study. 
Variables 
RR TEAM INSTRUCTIONS:​ ​The preregistration form divides variables across three questions: 
manipulated variables, measured variables, and indices (i.e. analytic variables derived from raw 
variables). For existing data replications, only fill out the “Measured variables’ and ‘Indices’ 
sections. Please do not fill out anything in the ‘Manipulated variables’ section.  
 
The raw data of any transformed variable (e.g. reaction time → log reaction time) or any created 
index should be defined in the ‘Measured variables’ section. Details regarding the variable 
transformation should be specified in the ‘Transformations’ section. Details regarding the 
creation of an index should be specified in the ‘Indices’ section.  
 
Across these questions, you should define all variables that will later be used during your 
analysis (including data preparation/processing). You can describe all variables in the 
preregistration and/or summarize and link to a ​data dictionary​ (codebook) in your repository to 
answer these questions. 
 
If you will share data from your replication, this is also the place to state whether any variables 
will be removed prior to sharing the dataset (e.g. to reduce risk of participant identification or 
comply with copyright restrictions on scale items.)  
 
16. Manipulated variables 
RR TEAM INSTRUCTIONS:​ ​Manipulated variables in this preregistration refer specifically to 
variables that have been randomly assigned in an experiment. The use of data from an 
experiment should be rare in existing data replications. If your existing data replication relies on 
 

 
experimental data, please document each manipulated variable as a measured variable, and 
use the codebook to indicate what each level of the variable corresponds to (e.g. participants 
assigned to the treatment condition = 1; participants assigned to the control condition = 0). The 
default language in bold below has been copied into all existing data replication preregistrations.  
 
N/A -- not documented for existing data replications. 
 
17. Measured variables 
RR TEAM INSTRUCTIONS:​ ​Please use this section to document each variable that was used 
in the original study’s analysis and the role it served (e.g. dependent variable, control variable, 
sample parameter, etc). For each variable, provide the description of the variable offered in the 
paper and/or codebook of the original study, the variable in the replication dataset that it 
corresponds to, and explain any deviations between the two. In cases where an equivalent 
replication variable was not found, explain how, if at all, you expect it will affect the replication 
attempt. In cases where you are adding a variable that was not present in the original study, 
please explicitly state that you are doing so, and explain how, if at all, you expect it will affect the 
replication attempt. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration surface all of the variables needed to replicate the focal 
analysis? 
●
Are deviations between the original variables and replication variables documented 
when needed? 
 
A link to an updated data dictionary can be found here:​ ​https://osf.io/v2yb5/ 
The study required two steps: conducting the latent class analysis and testing whether the 
classes differ on the variable “evolved.” 
For the latent class analysis, I manipulated three groups of variables: 
For the first group of variables, ‘don’t know’ is treated as an informative missing and left in the 
analysis. ‘No answer’ and ‘not applicable’ are treated as missing. 
Hotcore, radioact, boyorgrl,  lasers,  electron, viruses, earthsun, condrift, bigbang, evolved, 
expdesgn, odds1, odds2, scistudy 
  
 

 
For the second group of variables, ‘don’t know,’ ‘no answer,’ and ‘not applicable’ were all 
treated as missing. Toofast was also reverse coded. 
The recoded values are as follows: 
Nexgen, and advfront – 1 = Strongly agree, 2= agree, 3= disagree 4=strongly disagree 
Scibnfts – 0 = hamfuls results greater than benefits, 2 = about equal, 4 = benefits greater than 
harm 
Toofast – 1 = strongly disagree, 2 = disagree, 3= agree, 4= strongly agree 
  
For the third group of variables, I recoded the variable ‘bible.’ The variable correspond directly 
to the responses to the original variable. All respondents who did not choose one of the three 
possible responses (word of God, inspired by the word of God, fables) were coded as missing. 
Bible – 1 = Word of God, 2= Inspired by the word of God, 3 = fables, legends, and moral 
precepts 
Reliten – 4 = strong, 3=not very strong, 2 = somewhat strong, 1 = no religion 
For the t-test, I created a set of variables reflecting the class that respondents are assigned 
to. 
Variables created from the latent class model include: traditional, modern, and post-secular. To 
decide which class is which, I check the means of the manifest variables (variables included in 
the Latent Class Model) and label them with the following principles: 
1.​     ​The modern category will have the weakest religious beliefs and high scores for 
science literacy and attitude questions. 
2.​     ​The traditionalist category will have strong religious beliefs and low scores for all 
science literacy and attitude questions. 
3.​     ​The post-secular category will have strong religious beliefs and high scores for many 
science and literacy questions, but not all.  
I draw these descriptions from the original text, page 103. 
 
 
 

 
VARIABLE NAME 
●
[Use in the analysis] 
●
[Description from the original study] 
●
[Variables used in the replication (if it needs to be constructed from multiple measures, 
include all of them here)] 
●
[Deviations between the original study and the replication study] 
 
 
NEXTGEN 
·​       ​Manifest variable 
·​       ​Perspectives on science encompass attitudes about science as well as knowledge of 
scientific concepts and methods. We examine attitudes about science using survey 
questions commonly used to measure public appreciation of science (Miller 2004). 
·​       ​NEXTGEN (same variable, newer waves of the data) 
·​       ​No deviations. 
  
TOOFAST 
·​       ​Manifest variable 
·​       ​Perspectives on science encompass attitudes about science as well as knowledge of 
scientific concepts and methods. We examine attitudes about science using survey 
questions commonly used to measure public appreciation of science (Miller 2004). 
·​       ​TOOFAST (same variable, newer waves of the data) 
·​       ​No deviations. 
  
ADVFRONT 
·​       ​Manifest variable 
·​       ​Perspectives on science encompass attitudes about science as well as knowledge of 
scientific concepts and methods. We examine attitudes about science using survey 
questions commonly used to measure public appreciation of science (Miller 2004). 
·​       ​ADVFRONT  (same variable, newer waves of the data) 
·​       ​(This differs from the data prepared by Marco Ramljak. Based on the questions from 
table two of the original paper, I believe this is the question used in the original analysis, 
rather than NATSCI.) 
  
SCIBNFTS 
·​       ​Manifest variable 
·​       ​Perspectives on science encompass attitudes about science as well as knowledge of 
scientific concepts and methods. We examine attitudes about science using survey 
questions commonly used to measure public appreciation of science (Miller 2004). 
·​       ​SCIBNFTS (same variable, newer waves of the data) 
·​       ​No deviations. 
  
HOTCORE 
·​       ​Manifest variable 
 

 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​HOTCORE (same variable, newer waves of the data) 
·​       ​No deviations 
  
RADIOACT 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​RADIOACT (same variable, newer waves of the data) 
·​       ​No deviations 
  
BOYORGRL 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​BOYORGRL (same variable, newer waves of the data) 
·​       ​No deviations 
  
LASERS 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​LASERS (same variable, newer waves of the data) 
·​       ​No deviations 
  
ELECTRON 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​ELECTRON (same variable, newer waves of the data) 
·​       ​No deviations 
  
VIRUSES 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​VIRUSES (same variable, newer waves of the data) 
·​       ​No deviations 
  
EARTHSUN 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​EARTHSUN (same variable, newer waves of the data) 
 

 
·​       ​No deviations 
  
CONDRIFT 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​CONDRIFT (same variable, newer waves of the data) 
·​       ​No deviations 
  
BIGBANG 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​BIGBANG (same variable, newer waves of the data) 
·​       ​No deviations 
  
EVOLVED 
·​       ​Manifest variable and ​outcome of the t-test​ (the requested hypothesis). 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​EVOLVED (same variable, newer waves of the data) 
·​       ​No deviations 
  
EXPDESGN 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​EXPDESGN (same variable, newer waves of the data) 
·​       ​No deviations 
  
ODDS1 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​ODDS1 (same variable, newer waves of the data) 
·​       ​No deviations 
  
ODDS2 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​ODDS2 (same variable, newer waves of the data) 
·​       ​No deviations 
  
SCISTUDY 
 

 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We examine knowledge of science using a 
series of 14 quiz-style questions modeled as binary variables. 
·​       ​SCISTUDY (same variable, newer waves of the data) 
·​       ​No deviations 
  
RELITEN 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: We also analyze individuals’ religiosity as 
an ordinal variable based on a question that asked respondents to rate the strength of their 
religious beliefs on a fourpoint scale, where higher scores correspond to stronger belief. 
·​       ​RELITEN 
·​       ​No deviations 
  
BIBLE 
·​       ​Manifest variable 
·​       ​Description from O’Brien and Noy 2015: To measure perspectives on religion, we 
analyze responses to a question that asked whether the Bible is (1) the actual word of 
God, (2) inspired by the word of God, or (3) filled with myths and fables. This item, 
which we model as a nominal variable, is frequently used to measure views of certain 
religious interpretations of the world (Davis and Robinson 1999). 
·​       ​BIBLE (same variable, newer waves of the data) 
·​       ​No deviations 
  
Predclass 
·​       ​Outcome of Latent Class Model 
·​       ​This variable is created by using the predicted probability that a respondent will fall 
into a latent class. Labels for the classes are generated using the principles listed in 
section 16. 
·​       ​Variable not included in the original dataset. 
·​       ​There are no significant deviations between this variable and the coding of the original 
study. 
  
PostsecVsTrad 
·​       ​Recode of predclass 
·​       ​For this variable, post-secularists are coded as ‘1’ and traditionalists are coded as ‘0’. I 
create this variable to conduct a t-test for the two groups. 
·​       ​Variable not included in the original dataset. 
 
18. Indices 
RR TEAM INSTRUCTIONS:​ ​If any of the measured variables described in Section 17 will be 
combined into a composite measure (including simply a mean), describe in detail what 
 

 
measures you will use and how they will be combined. Please be sure this preregistration 
includes a link to a clearly commented script that constructs the index according to the narrative. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
 Does the preregistration specify each of the composite measures (e.g. mean scores, 
factor scores) that are needed for the focal analysis, and which of the measured 
variables in Section 17 are used in each one (e.g. the happiness, joy, and satisfaction 
items will be used to create the ‘positive feelings’ measure)? 
●
Does the preregistration link to a clearly commented script that constructs the indices 
according to the narrative description? 
 
  
Three measures are created from the results of the Latent Class Analysis: traditional, modern, 
and post-secular. Respondents are categorized into one of these three options if the probability of 
them falling into that category (generated from the Latent Class Analysis) is greater than the 
other two categories. LCA only provides the probability of falling into a class; it does not label 
the categories. To decipher the labels for the categories, I look at the conditional means from the 
LCA and match them according to the principles from O’Brien and Noy 2015. See section 16 of 
this pre-registration for the conditions used to label the categories.  
 
Analysis Plan 
19. Statistical models 
RR TEAM INSTRUCTIONS:​ ​This section should describe in detail the analysis that will be 
performed to replicate the focal result. This analysis must align as closely as possible with the 
original study’s analysis, even if you have identified limitations in the original study. The level of 
detail should allow anyone to reproduce your analyses from your description below. Examples 
of what should be specified: the model; each variable; adjustments made to the standard errors 
and to case weighting; additional analyses that are required to set up the focal analysis; and the 
software used. 
 
Beyond the replication of the focal analysis from the original study, it is at your discretion to test 
the claim using other analytic approaches as a check of the robustness of the claim. The 
original test should be listed first and be clearly distinguished from any other tests. If you are 
testing additional confirmatory hypotheses, describe them in the same order as you numbered 
them in the “Hypotheses” section above and make clear reference to the specific hypothesis 
being tested for each. 
 

 
 
Please provide a link to a clearly commented script that performs the analysis described in the 
narrative provided below. Our preference is that this be either an R script or a script from 
another language that similarly allows for open and reproducible analyses. Please let the 
SCORE team know if this is not possible. Please also test that the code runs without error on a 
random subset of 5% of the replication dataset, and provide verification that the code has 
produced a sensible result below (a screenshot of the results is preferable). Finally, please 
confirm that you have only developed and tested your analysis plan and code using 5% of the 
data. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration specify which statistical model will be used to provide the ‘focal 
evidence’ for the SCORE test (e.g. a regression coefficient in a larger multiple regression 
model), and does it correspond closely to the model and evidence from the original 
study? 
●
Does the preregistration describe each variable that will be included in the focal analysis, 
and what role each variable has (e.g. dependent variable, independent variable)? 
●
Does the preregistration include a detailed specification of the focal analysis, including 
interactions, lagged terms, controls, etc., in both narrative form and in a clearly 
commented script? 
●
Does the preregistration verify that the code runs without error on a random subset of 
the replication dataset? 
 
This statement confirms that only 5% of the data have been randomly sampled in 
developing the analysis plan and code contained in this preregistration​. 
H*: Members of the post-secular category were significantly less likely than members of the 
traditional group to respond that humans evolved from other animals (3 percent, significant at p 
< 0.05 on a two-tailed test, see Table 2, rightmost column). 
 I do not conduct all analyses in the original paper. Instead, I conduct Latent Class Analysis and 
run a t-test comparing the responses to EVOLVED by post-secularists and traditionalists. 
To test this, I first conduct Latent Class Analysis using the responses from 20 survey questions. I 
conduct this analysis using Stata 16’s ‘lclass’ option in ‘gsem’. See the uploaded do-file for the 
complete analysis code (​https://osf.io/q7muh/​). The variables I include are listed in section 17 of 
this preregistration. Recodes are described in section 20 of this preregistration. How missing data 
are handled is described in section 23 of this preregistration. 
Following listwise deletion and deleting years per the relevant analysis, I create a 5% 
subsample of the dataset for these analyses. 
 

 
I run the Latent Class Models. I assign ‘bible’ as a multinomial logistic predictor, I assign 
‘scistudy,’ ‘nextgen,’ ‘toofast,’ ‘advfront,’ ‘scibnfts,’ ‘scistudy,’ and ‘reliten’ as ordinal logistic 
predictors, and I assign all other manifest variables (see section 17) as binomial logistic 
predictors. I randomly draw 15 start values to determine where the model should begin testing 
the sample space. I create three categorical classes. No adjustments are made to the standard 
errors. I weight the results using sample weights (wtss). 
After the model runs, I predict the probability that each respondent will fall into particular 
classes. I create a new variable (predclass) with three categories and assign respondents to a 
category depending on which class has the highest probability. 
I check the mean responses for each of the manifest variables by the variable ‘predclass’. From 
an ocular examination, and using the principles listed in section 17, I assign labels to ‘predclass’ 
(Modern, Post-secular, or Traditional).  
I create a new variable (PostsecVsTrad) which is coded as 1 if respondents are in the post-secular 
category and 0 if respondents are in the traditional category. 
Finally, I conduct a t-test on responses to the evolution question (Human beings developed from 
earlier species of animals?) by traditionalist and post-secularists.  
20. Transformations 
RR TEAM INSTRUCTIONS:​ ​This section should describe how any of the measured variables or 
composite measures mentioned above will be transformed prior to the analyses listed in Section 
19. These are adjustments made to variables ​after​ measurement or measure creation, and 
might include centering, logging, lagging, rescaling etc. Please provide enough detail such that 
anyone else could reproduce the transformations based on the description below. Please be 
sure this preregistration includes a link to a clearly commented script that performs the 
transformations described in the narrative provided below. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration specify which of the measured variables or composite 
measures will need to be transformed prior to the focal analysis? 
●
For each variable needing transformation, does the preregistration adequately describe 
the transformations, including any centering, logging, lagging, recoding, or 
implementation of a coding scheme for categorical variables? 
●
Does the preregistration link to a clearly commented script that performs each 
transformation? 
 
 

 
The do-file which contains the recode can be found here: (​https://osf.io/q7muh/​). 
For the following variables, I recode correct responses as ‘1’, incorrect responses as zero ‘0’, and 
don’t know as ‘0’. Other responses are recoded as missing. Those variables include: hotcore, 
radioact, boyorgrl, lasers, electron viruses earthsun condrift bigbang evolved expdesgn odds1 
odds2. 
I code scientific study (scistudy) with its original three possible responses. Don't know (8), No 
answer (9), and not applicable (0) are coded as missing. 
Scale response variables (nextgen, toofast, advfront, scibnfts) are recoded to account for missing. 
Don't know (8), no answer (9), and not applicable (0) are recoded as missing.  TOOFAST must 
also be reverse coded. 
Bible is recoded using its original three responses (word of God, inspired by the word of God, 
and fables), while don't know (8), no answer (9), and not applicable (0) are coded as missing. 
Religious affiliation (reliten) is recoded such that don't know (8), no answer (9), and not 
applicable (0) are coded as missing.  
 
21. Inference criteria 
RR TEAM INSTRUCTIONS:​ ​This section describes the precise criteria that will be used to 
assess whether the hypotheses listed above were confirmed by the analyses in Section 19. The 
default language below only applies to the test of the SCORE claim, ​H*​. It is at your discretion to 
describe the inferential criteria you will use for any additional analyses. They need not rely on 
p-values and/or the same alpha level we have specified for ​H*​.  
 
If the additional analyses will use multiple comparisons, the inference criteria is a question with 
few “wrong” answers. In other words, transparency is more important than any specific method 
of controlling the false discovery rate or false error rate. One may state an intention to report all 
tests conducted or one may conduct a specific correction procedure; either strategy is 
acceptable. 
 
Criteria for a successful replication attempt for the SCORE project is a statistically significant 
effect (alpha = .05, one tailed) in the same pattern as the original study on the focal hypothesis 
test (​H*​). For this study, this criteria is met by a one-tailed t-test showing that, compared to 
traditionalists, post-secularists are significantly less likely to correctly state that humans evolved 
from earlier species of animals. 
 

 
  
For this study, the request from OSF asks if post-secularists were significantly less than 
traditionalists, implying a one-tailed test. The original paper uses a two-tailed t-test for significant 
differences between the categories (p<.05). I will include results to both analyses, although I 
follow SCORE’s replication criterion and define a successful replication as a significant 
one-tailed test (p<.05). 
In the following results, “group 1” is post-secularists. 
 
Results from a 5% sample of years 2012, 2014, 2016, and 2018 
 
 
Results from a 5% sample of all years: 
 

 
 
 
Results from a 5% sample of the years included in the original paper (2006, 2008, and 2010)
 
 
 

 
22. Data exclusion 
RR TEAM INSTRUCTIONS:​ ​The section below should describe the rules you will follow to 
exclude collected cases from the analyses described in Section 19. Note that this refers to 
exclusions ​after​ the creation of the replication dataset; exclusion criteria that prevent a case 
from entering the replication dataset in the first place should be detailed in the ‘Data Collection 
Procedure’ section above. Please be as detailed as possible in describing the rules you will 
follow (e.g. What is the specific definition of outliers you will use? Exactly how many attention 
checks does a participant need to fail before their removal from the analytic sample?). 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration comment on whether any cases included in the replication 
dataset will be excluded prior to data analysis? 
●
If yes, does the preregistration provided detailed instructions on how the exclusions will 
be performed (e.g. Is the definition of outlier provided? Is the number of attention checks 
failed before a participant is excluded specified?) 
 
n/a 
23. Missing data 
RR TEAM INSTRUCTIONS:​ ​The section below should describe how missing or incomplete data 
will be handled. Please be as detailed as possible in describing the exact procedures you will 
follow (e.g. last value carried forward; mean imputation) and any software required (e.g. We will 
use Amelia II in R to perform the imputation). 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration comment on how missing or incomplete data will be addressed 
(e.g. casewise removal, missing data imputation)? 
●
If applicable, does the preregistration specify how many missing variables will lead to a 
case’s removal (e.g. If a subject does not complete any of the three indices of tastiness, 
that subject will not be included in the analysis.)? 
●
If applicable, does the preregistration describe how missing data imputation will be 
performed, including relevant software? 
 
In this analysis, I conducted three analyses: the first only included observations from GSS survey 
years 2012, 2014, 2016, and 2018; the second included all observations; the third included the 
years from the original paper (2006, 2008, and 2010). Variables with missing data are removed 
from the analytic sample using listwise deletion. One missing response to any question justifies 
removal. A do-file detailing how missing values were removed can be found here: 
(​https://osf.io/q7muh/​). 
 

 
The original paper also uses listwise deletion. I do not perform imputation, similar to the original 
study. I test my coding scheme against the original paper’s by checking the number of 
observations for each year for the years used in the original paper. The sample sizes from the 
original study were: 1,563 from 2006; 988 from 2008; and 350 from 2010. The sample sizes 
from my coding scheme were: 1,608 from 2006, 1,016 from 2008, and 367 from 2010. My 
analytic sample has 90 extra observations. I do not believe this difference will impact the focal 
hypothesis test.  
 
24. Exploratory analysis (Optional) 
RR TEAM INSTRUCTIONS:​ ​If you plan to explore your data set to look for unexpected 
differences or relationships, you may describe those tests here. An exploratory test is any test 
where a prediction is not made up front, or there are multiple possible tests that you are going to 
use. A statistically significant finding in an exploratory test is a great way to form a new 
confirmatory hypothesis, which could be registered at a later time. If any exploratory analyses 
involve additions to the data collection procedure beyond what was performed in the original 
study (e.g. additional items on the survey; running another condition in the experiment), please 
describe them below. 
n/a 
No exploratory analyses were or will be conducted. 
 
25. Other 
RR TEAM INSTRUCTIONS:​ ​This section serves two purposes. First, please​ ​use this section to 
discuss any features of your replication plan that are not discussed elsewhere. Literature cited, 
disclosures of any related work such as replications or work that uses the same data, plans to 
make your data and materials public, or other context that will be helpful for future readers 
would be appropriate here. Second, please also re-surface any major deviations from earlier in 
the preregistration that you expect a reasonable reviewer could flag for concern. Give a 
summary of these deviations, focusing on larger changes and any possible challenges for 
comparing the results of the original and replication study. 
 
Specific points to keep in mind (please also consult the ​Reviewer Criteria​): 
●
Does the preregistration reference other sections of the preregistration where substantial 
deviations from the original study have been described (including deviations due to 
differences in location or time compared to the original study)?  
●
Does the preregistration comment on plans to make the data and materials from the 
replication study public? 
 
 
 

 
I tested the analytic sample my code produces using the same years as the original study. I found 
that my sample contains 90 more respondents than the original study. It’s unclear how the 
original authors decided to remove those observations. I do not believe this difference changes 
the outcome of the focal hypothesis test. 
I currently have no plans to make the data and materials from this replication study public, 
outside of the general policies of OSF.  
 
 
 
 
 

 
Final review checklist 
REVIEWER INSTRUCTIONS: ​For the following questions, reviewers please indicate whether 
you can ‘sign off’ on the following items by adding a comment. You can update this response as 
the lab moves through revisions during the review period! 
 
●
Included in this pre-registration are specific materials needed to create a replication 
dataset: 
○
Is the final replication dataset that the research team constructed suitable for 
performing a high-quality, good-faith replication of the focal claim selected from 
the original study? 
○
Is the procedure for constructing the final replication dataset sufficiently 
documented that an independent researcher could construct the same dataset 
following the procedures and code they lay out? 
●
Included with this pre-registration is a narrative description of how the replication dataset 
will be used to perform the focal replication analysis, as well as the specific analytic 
scripts/code/syntax that will be used: 
○
Is the analysis plan (including code) that’s documented in the preregistration 
consistent with a high-quality, good-faith replication of the focal claim selected 
from the original study? 
○
Has the data analyst demonstrated that the analysis code works as expected on 
a random 5% of the final replication dataset? 
●
I have reviewed all sections of this pre-registration, and I believe it represents a 
good-faith replication attempt of the original focal claim. 
 
 

=== END OF REFERENCE JSON WITH THE CORRECT INFO ===

Please return your evaluation as a JSON object where each key is a specific component from the original JSON. For example:
{
    "hypothesis": {
    "score": [your scoring according to the instruction],
    "explanation": [reasoning for your scoring.]
    },
    "data_plan.source_type": {
    "score": [your scoring according to the instruction],
    "explanation": [reasoning for your scoring.]
    },
    ...
}
Output Requirements:\n- Return a valid JSON object only.\n- Do NOT wrap the output in markdown (no ```json).\n- Do NOT include extra text, commentary, or notes.\n\n Ensure accuracy and completeness.\n- Strictly use provided sources as specified.


