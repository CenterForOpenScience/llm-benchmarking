# LLM Benchmarking Project

Welcome to the official repository for the **LLM Benchmarking Project**, led by the Center for Open Science (COS). This project aims to evaluate the capabilities of large language model (LLM) agents across key components of the scientific research lifecycle, including **replication**, **peer review**, and **research design**.

## ğŸ” What This Project Is About

We are developing a modular benchmark framework to assess whether and how LLM agents can:

* **Replicate published scientific findings**
* **Evaluate the quality and credibility of research outputs**
* **Generate valid and meaningful research designs**

This work builds on the conceptual structure outlined in our Open Philanthropy grant, emphasizing real-world relevance, task diversity, and community participation.

## ğŸš§ Current Status

This repository is in **active development**. Right now, it hosts internal work on:

* Task definitions for replication benchmarking
* Agent development and evaluation pipelines
* Experimental scaffolding for testing and refining agent performance

Over time, we will open up parts of this repo for **community use and feedback**, including:

* Evaluation harnesses
* Benchmarks and datasets
* Contribution guidelines for task submissions and agent evaluation strategies


## Project Structure
```
llm-benchmarking/
â”‚
â”œâ”€â”€ info_extractor/
â”‚   â”œâ”€â”€ extractor.py
â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”œâ”€â”€ prompt_builder.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ validator/
â”‚   â”œâ”€â”€ extract_from_human_replication_study.py
â”‚   â”œâ”€â”€ compare_outputs.py                        
â”‚   â””â”€â”€ README.md                                 
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ info_extractor_instructions.json
â”‚   â”œâ”€â”€ interpret_schema.json
â”‚   â”œâ”€â”€ post_registration_schema.json
â”‚   â””â”€â”€ replication_info_schema.json
â”‚
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ initial_details_easy.txt
â”‚   â””â”€â”€ initial_details_medium_hard.txt             
â”‚
â”œâ”€â”€ constants.py
â”œâ”€â”€ extract_human_replication_info.py
â”œâ”€â”€ main.py
â”œâ”€â”€ README.md
â””â”€â”€ validate_info_extractor.py


```

## ğŸ§° Installation
1. Clone repository:
   ```bash
   git clone https://github.com/CenterForOpenScience/llm-benchmarking.git
   cd llm-benchmarking
   ```

2. Install dependencies:
   ```bash
   pip install openai pymupdf pandas python-docx dotenv pyreadr
   ```

3. Configure API key in `constants.py`

## ğŸ”§ Usage

### Info Extractor Module

This module runs LLM-based extraction of structured metadata from original and replication studies (based on the difficulty level).

```bash
# Stage 1: Extract from original study
python main.py --study_path ./studies/case_study_1 --stage stage_1 --difficulty easy

# Stage 2: Extract from replication study
python main.py --study_path ./studies/case_study_1 --stage stage_2 --difficulty easy
```

**Arguments:**
* `--study_path`: Path to the study folder
* `--stage`: `"stage_1"` for original, `"stage_2"` for replication
* `--difficulty`: `"easy"`, `"medium"`, or `"hard"`
* `--show-prompt`: Print the constructed LLM prompt for debugging

#### Output Files
* Stage 1 â†’ `post_registration.json`
* Stage 2 â†’ `replication_info.json`

#### Input File Requirements

##### Stage 1

| Difficulty | Required Files |
|------------|----------------|
| Easy | `initial_details_easy.txt`, `original_paper.pdf` |
| Medium | `initial_details_medium_hard.txt`, `original_paper.pdf` |
| Hard | `initial_details_medium_hard.txt`, `original_paper.pdf` |

##### Stage 2

| Difficulty | Required Files |
|------------|----------------|
| Easy | `initial_details_easy.txt`, `original_paper.pdf`, `post_registration.json`, `replication_data.csv` |
| Medium | `initial_details_medium_hard.txt`, `original_paper.pdf`, `post_registration.json` |
| Hard | `initial_details_medium_hard.txt`, `original_paper.pdf`, `post_registration.json` |

### Validator Module

This module validates whether the metadata extracted by the info extractor matches what is expected based on human-annotated metadata.
- We use an LLM (GPT4o) to compare the extracted info (`extracted_json.json`) against the human-annotated ground-truth (`expected_json.json`).
- We use the same proposed evaluation rubrics in the task design as prompt to the LLM-as-judge and ask it to assign a score for the extracted info (can be found under `templates/prompts/extract_eval.txt`.

```bash
python evaluate_extract_info.py \
  --extracted_json_path path/to/extracted_json.json \
  --expected_json_path path/to/expected_json.json \
  --output_path path/to/study_dir/llm_eval.json
```
##### Output
* JSON formatted evaluation of the extracted metadata
* prompt for traceability (`logs/` directory)

## ğŸ” Access and Permissions

This repository is managed under the **COS GitHub organization**, with:

* **Admin access** retained by COS staff
* **Write or maintain access** granted to approved external collaborators


## ğŸ“„ License

All content in this repository is shared under the [Apache License 2.0](LICENSE)

## ğŸ‘¥ Contributors

Core team members from COS, plus external partners from Old Dominion University, Pennsylvania State University, and University of Notre Dame  specializing in:

* Agent development
* Benchmark design
* Open Science Research

## ğŸ“¬ Contact

For questions please contact:

**Shakhlo Nematova**
Research Scientist
[shakhlo@cos.io](mailto:shakhlo@cos.io)

---
